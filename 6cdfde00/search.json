[
  {
    "objectID": "slides/index.html#what-is-mlops",
    "href": "slides/index.html#what-is-mlops",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "What is MLOps",
    "text": "What is MLOps"
  },
  {
    "objectID": "slides/index.html#mlops-and-devops",
    "href": "slides/index.html#mlops-and-devops",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "MLOps and DevOps",
    "text": "MLOps and DevOps\n\nDevOps seeks to shorten the cycle between development and operationalizing code.\n\nDEVOPS CYCLE"
  },
  {
    "objectID": "slides/index.html#mlops-and-devops-1",
    "href": "slides/index.html#mlops-and-devops-1",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "MLOps and DevOps",
    "text": "MLOps and DevOps\nMLOPS CYCLE"
  },
  {
    "objectID": "slides/index.html#tasks-and-tools",
    "href": "slides/index.html#tasks-and-tools",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Tasks and tools",
    "text": "Tasks and tools\n\nthere’s not always a 1:1 ratio of task:tool"
  },
  {
    "objectID": "slides/index.html#data-gathering",
    "href": "slides/index.html#data-gathering",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Data gathering",
    "text": "Data gathering"
  },
  {
    "objectID": "slides/index.html#exploratory-data-analysis",
    "href": "slides/index.html#exploratory-data-analysis",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis"
  },
  {
    "objectID": "slides/index.html#exploratory-data-analysis-1",
    "href": "slides/index.html#exploratory-data-analysis-1",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis"
  },
  {
    "objectID": "slides/index.html#modeling",
    "href": "slides/index.html#modeling",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Modeling",
    "text": "Modeling"
  },
  {
    "objectID": "slides/index.html#modeling-1",
    "href": "slides/index.html#modeling-1",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Modeling",
    "text": "Modeling"
  },
  {
    "objectID": "slides/index.html#profiling-the-space",
    "href": "slides/index.html#profiling-the-space",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Profiling the space",
    "text": "Profiling the space\n\nrecency"
  },
  {
    "objectID": "slides/index.html#profiling-the-space-1",
    "href": "slides/index.html#profiling-the-space-1",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Profiling the space",
    "text": "Profiling the space\n\nrecency\nsize"
  },
  {
    "objectID": "slides/index.html#profiling-the-space-2",
    "href": "slides/index.html#profiling-the-space-2",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Profiling the space",
    "text": "Profiling the space\n\nrecency\nsize\ncontribution rate"
  },
  {
    "objectID": "slides/index.html#a-space-of-learners",
    "href": "slides/index.html#a-space-of-learners",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "A space of learners",
    "text": "A space of learners\n\nmuch of this exploration leads to the fact that MLOps is not an established space. people are just starting to learn, experiment, try out examples, teach others, etc. maybe, because this a space is a big conglomeration of established principles like devops and machine learning and reproducible work, there is no traditional pathway where you can learn definitions. MLOps practices are generally only implemented at scale, so mocking out large systems makes this a difficult space."
  },
  {
    "objectID": "slides/index.html#cloud-tooling-prevalence",
    "href": "slides/index.html#cloud-tooling-prevalence",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Cloud tooling prevalence",
    "text": "Cloud tooling prevalence"
  },
  {
    "objectID": "project/04_modeling.html",
    "href": "project/04_modeling.html",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "",
    "text": "import pandas as pd\nfrom siuba import *\nfrom siuba.siu import call\nfrom plotnine import *\nimport json\nimport matplotlib.pyplot as pp\nimport tidytext\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport string\nimport random\nimport nltk\nfrom nltk import FreqDist\nfrom nltk import ngrams\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nnp.random.seed(500)\n\n\nimport pins\n\nboard = pins.board_folder(\".\")\n\nnumeric_df = board.pin_read(\"train_data\")\n\n\nnumeric_df.drop(columns=[\"allow_forking\"], inplace=True)\nnonone = (numeric_df \n    >> select(_.description)\n    >> filter(-_.description.isin([None])))\n\n\nimport re\ndef remove_emojis(data):\n    emoj = re.compile(\"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002500-\\U00002BEF\"  # chinese char\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        u\"\\U0001f926-\\U0001f937\"\n        u\"\\U00010000-\\U0010ffff\"\n        u\"\\u2640-\\u2642\" \n        u\"\\u2600-\\u2B55\"\n        u\"\\u200d\"\n        u\"\\u23cf\"\n        u\"\\u23e9\"\n        u\"\\u231a\"\n        u\"\\ufe0f\"  # dingbats\n        u\"\\u3030\"\n                      \"]+\", re.UNICODE)\n    return re.sub(emoj, '', data)\n\nfrom sklearn.feature_extraction import text\nstop_words = text.ENGLISH_STOP_WORDS.union(frozenset({'b', 's'}))\n\n\ntokenizer = RegexpTokenizer(r'\\w+')\nsents  = nonone.description.to_list()\n\nfor sent in sents:\n    remove_emojis(sent)\n\n# Vectorize document using TF-IDF\ntfidf = TfidfVectorizer(lowercase=True,\n                        stop_words=stop_words,\n                        ngram_range = (1,1),\n                        tokenizer = tokenizer.tokenize)\n\n# Fit and Transform the documents\ntrain_data = tfidf.fit_transform(sents) \n\n\ndef get_topics(lda_model, tfidf_model, data):\n    terms = tfidf_model.get_feature_names_out()\n    # Fit and Transform SVD model on data\n    lda_matrix = lda_model.fit_transform(data)\n    lda_components = lda_model.components_\n\n    for index, component in enumerate(lda_components):\n        zipped = zip(terms, component)\n        top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:7]\n        top_terms_list=list(dict(top_terms_key).keys())\n        print(\"Topic \"+str(index)+\": \",top_terms_list)\n    \n    return lda_matrix, lda_components\n\n\n# visualize topics\nimport matplotlib.pyplot as plt\n\ndef plot_top_words(model, feature_names, n_top_words, title):\n    fig, axes = plt.subplots(5, 4, figsize=(30, 30), sharex=True)\n    axes = axes.flatten()\n    for topic_idx, topic in enumerate(model.components_):\n        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n        top_features = [feature_names[i] for i in top_features_ind]\n        weights = topic[top_features_ind]\n\n        ax = axes[topic_idx]\n        ax.barh(top_features, weights, height=0.7)\n        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 28})\n        ax.invert_yaxis()\n        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n        for i in \"top right left\".split():\n            ax.spines[i].set_visible(False)\n        fig.suptitle(title, fontsize=40)\n\n    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n    plt.show()\n\n\n# https://stackoverflow.com/questions/47370795/pca-on-sklearn-how-to-interpret-pca-components\n# good PCA refresher https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e\n\n\ndef pca_plot(transformed_data, components):\n\n    x = transformed_data[:, 0]\n    y = transformed_data[:, 1]\n    n = components.shape[0]\n\n    plt.scatter(x, y)\n    plt.xlabel(\"PC{}\".format(1))\n    plt.ylabel(\"PC{}\".format(2))\n    plt.grid()\n    \n    for i in range(n):\n        plt.arrow(0, 0, components[i, 0], components[i, 1], color=\"black\", alpha=0.5)\n        plt.text(\n            components[i, 0] * 1.3,\n            components[i, 1] * 1.3,\n            \"Var \" + str(i + 1),\n            color=\"black\",\n            ha=\"center\",\n            va=\"center\",\n        )\n\ndef most_important_feature(components, data):\n    most_important = [np.abs(components[i]).argmax() for i in range(components.shape[0])]\n\n    initial_feature_names = data.columns\n\n    most_important_names = [initial_feature_names[most_important[i]] for i in range(components.shape[0])]\n\n    dic = {'PC{}'.format(i): most_important_names[i] for i in range(components.shape[0])}\n\n\n    return pd.DataFrame(dic.items())\n\n\nLDA\n\n# model_05=LatentDirichletAllocation(n_components=5)\n# get_topics(model_05, tfidf, train_data)\n\n\n# model_10=LatentDirichletAllocation(n_components=10)\n# get_topics(model_10, tfidf, train_data)\n\n\n# model_12=LatentDirichletAllocation(n_components=12)\n# get_topics(model_12, tfidf, train_data)\n\n\nmodel_20=LatentDirichletAllocation(n_components=20)\nmatrix, components = get_topics(model_20, tfidf, train_data)\n\nTopic 0:  ['end', 'ci', 'sagemaker', 'cd', 'pipeline', 'mlops', 'store']\nTopic 1:  ['learning', 'mlops', 'machine', 'azure', 'python', 'projects', 'library']\nTopic 2:  ['ml', 'code', 'learning', 'project', 'using', 'machine', 'repository']\nTopic 3:  ['docker', 'tutorial', 'image', 'mlflow', 'mlops', 'airflow', 'internship']\nTopic 4:  ['example', 'pytorch', 'version', 'deep', 'kubernetes', 'deploy', 'learning']\nTopic 5:  ['model', 'dvc', '66daysofdata', 'app', 'data', 'triton', 'ml']\nTopic 6:  ['deeplearning', 'specialization', 'ai', 'ml', 'learning', 'mlops', 'machine']\nTopic 7:  ['end', 'learning', 'machine', 'platform', 'data', 'mlops', 'kubeflow']\nTopic 8:  ['deployment', 'model', 'ai', 'cloud', 'google', 'integration', 'ml']\nTopic 9:  ['tensorflow', 'mlops', 'project', 'extended', 'pipelines', 'ml', 'tool']\nTopic 10:  ['action', 'zoomcamp', 'github', 'mlops', 'repo', 'machine', 'learning']\nTopic 11:  ['model', 'list', 'curated', 'experiment', 'awesome', 'tracking', 'registry']\nTopic 12:  ['mlops', 'use', 'notebook', 'r', 'case', 'based', 'competition']\nTopic 13:  ['test', 'docs', 'data', 'ml', 'repository', 'tools', 'ifood']\nTopic 14:  ['mlops', 'end', 'cloud', 'tfx', 'using', 'vertex', 'google']\nTopic 15:  ['ml', 'api', 'core', 'python', 'client', 'polyaxon', 'project']\nTopic 16:  ['model', 'learning', 'machine', 'cloud', 'engineer', 'using', 'dl']\nTopic 17:  ['testing', 'training', 'fastapi', 'server', 'grafana', 'prometheus', 'mlflow']\nTopic 18:  ['zoomcamp', 'mlops', 'homework', 'datatalksclub', 'learning', 'codes', 'collection']\nTopic 19:  ['learning', 'model', 'ml', 'machine', 'project', 'azure', 'aws']\n\n\n\n# add topic to dataframe\ntopic = []\nfor n in range(matrix.shape[0]):\n    topic.append(matrix[n].argmax())\nnumeric_df[\"lda_topic\"] = topic\n\n\n## rerun with grid of different numbers of topics + document\nplot_top_words(model_20, tfidf.get_feature_names_out(), 20, \"Topics from repository descriptions\")\n\n/Users/isabelzimmerman/.pyenv/versions/pydemo/lib/python3.9/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 38283 (\\N{CJK UNIFIED IDEOGRAPH-958B}) missing from current font.\n/Users/isabelzimmerman/.pyenv/versions/pydemo/lib/python3.9/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 30330 (\\N{CJK UNIFIED IDEOGRAPH-767A}) missing from current font.\n/Users/isabelzimmerman/.pyenv/versions/pydemo/lib/python3.9/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12503 (\\N{KATAKANA LETTER PU}) missing from current font.\n/Users/isabelzimmerman/.pyenv/versions/pydemo/lib/python3.9/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12521 (\\N{KATAKANA LETTER RA}) missing from current font.\n/Users/isabelzimmerman/.pyenv/versions/pydemo/lib/python3.9/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12483 (\\N{KATAKANA LETTER SMALL TU}) missing from current font.\n/Users/isabelzimmerman/.pyenv/versions/pydemo/lib/python3.9/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12488 (\\N{KATAKANA LETTER TO}) missing from current font.\n/Users/isabelzimmerman/.pyenv/versions/pydemo/lib/python3.9/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12501 (\\N{KATAKANA LETTER HU}) missing from current font.\n/Users/isabelzimmerman/.pyenv/versions/pydemo/lib/python3.9/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12457 (\\N{KATAKANA LETTER SMALL O}) missing from current font.\n/Users/isabelzimmerman/.pyenv/versions/pydemo/lib/python3.9/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12540 (\\N{KATAKANA-HIRAGANA PROLONGED SOUND MARK}) missing from current font.\n/Users/isabelzimmerman/.pyenv/versions/pydemo/lib/python3.9/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12512 (\\N{KATAKANA LETTER MU}) missing from current font.\n/Users/isabelzimmerman/.pyenv/versions/pydemo/lib/python3.9/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 27169 (\\N{CJK UNIFIED IDEOGRAPH-6A21}) missing from current font.\n/Users/isabelzimmerman/.pyenv/versions/pydemo/lib/python3.9/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 22411 (\\N{CJK UNIFIED IDEOGRAPH-578B}) missing from current font.\n/Users/isabelzimmerman/.pyenv/versions/pydemo/lib/python3.9/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 35757 (\\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.\n/Users/isabelzimmerman/.pyenv/versions/pydemo/lib/python3.9/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 32451 (\\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.\n/Users/isabelzimmerman/.pyenv/versions/pydemo/lib/python3.9/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 33050 (\\N{CJK UNIFIED IDEOGRAPH-811A}) missing from current font.\n/Users/isabelzimmerman/.pyenv/versions/pydemo/lib/python3.9/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 25163 (\\N{CJK UNIFIED IDEOGRAPH-624B}) missing from current font.\n/Users/isabelzimmerman/.pyenv/versions/pydemo/lib/python3.9/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 26550 (\\N{CJK UNIFIED IDEOGRAPH-67B6}) missing from current font.\n\n\n\n\n\n\nnumeric_df.drop(columns=['description'], inplace=True)\n\n\n\nScaled PCA\n\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder().fit_transform(numeric_df['lda_topic'])\nscaled_numeric_df = StandardScaler().fit_transform(numeric_df)\npca_numeric_scaled = PCA(n_components=2).fit(scaled_numeric_df)\npca_numeric_scaled_output = pca_numeric_scaled.transform(scaled_numeric_df)\n\n\nfrom sklearn.pipeline import Pipeline\npipe = Pipeline(\n    [(\"le\", preprocessing.LabelEncoder()), \n    (\"scaler\", StandardScaler()), \n    (\"pca\", PCA()),\n    (\"dbscan\", DBSCAN())]\n)\n\n\npipe.fit(numeric_df)\n\nTypeError: fit_transform() takes 2 positional arguments but 3 were given\n\n\n\nplt.title(\"PCA of MLOps GitHub dataset, scaled numeric columns\")\n\npca_plot(pca_numeric_scaled_output, pca_numeric_scaled.components_) \nplt.show()\n\n\n\n\n\n\nDBSCAN\n\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics\n\ndb = DBSCAN(eps = 0.5, min_samples=5).fit(pca_numeric_scaled_output)\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint(\"Estimated number of clusters: %d\" % n_clusters_)\nprint(\"Estimated number of noise points: %d\" % n_noise_)\n\nEstimated number of clusters: 5\nEstimated number of noise points: 33\n\n\n\nunique_labels = set(labels)\ncore_samples_mask = np.zeros_like(labels, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\ncolors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = labels == k\n\n    xy = pca_numeric_scaled_output[class_member_mask & core_samples_mask]\n    plt.plot(\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=14,\n    )\n\n    xy = pca_numeric_scaled_output[class_member_mask & ~core_samples_mask]\n    plt.plot(\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\n\nplt.title(f\"DBSCAN estimated number of clusters: {n_clusters_}\")\nplt.show()\n\n\n\n\n\nnumeric_df['clustering_labels'] = db.fit_predict(pca_numeric_scaled_output)\n\n\nfrom sklearn import metrics\nmetrics.silhouette_score(pca_numeric_scaled_output, numeric_df['clustering_labels'])\n\n0.6235151746664813\n\n\ndoesn’t have overlapping clusters or mislabeled data points.\n\nnumeric_df.columns\n\nIndex(['stargazers_count', 'has_issues', 'has_projects', 'has_downloads',\n       'has_wiki', 'has_pages', 'has_discussions', 'open_issues_count',\n       'is_template', 'age_days', 'time_since_last_commit_days', 'lda_topic',\n       'clustering_labels'],\n      dtype='object')\n\n\n\n(numeric_df\n    >> group_by(_.clustering_labels)\n    >> summarize(\n        avg_stars = _.stargazers_count.mean(),\n        avg_has_issues = _.has_issues.mean(),\n        avg_has_projects = _.has_projects.mean(),\n        avg_has_downloads = _.has_downloads.mean(),\n        avg_has_wiki = _.has_wiki.mean(),\n        avg_has_pages = _.has_pages.mean(),\n        avg_has_discussions = _.has_discussions.mean(),\n        avg_open_issues_count = _.open_issues_count.mean(),\n        avg_is_template = _.is_template.mean(),\n        avg_age_days = _.age_days.mean(),\n        avg_last_commit = _.time_since_last_commit_days.mean(),\n        most_lda_topic = _.lda_topic.mode()[0] + 1\n    )\n)\n\n\n\n\n\n  \n    \n      \n      clustering_labels\n      avg_stars\n      avg_has_issues\n      avg_has_projects\n      avg_has_downloads\n      avg_has_wiki\n      avg_has_pages\n      avg_has_discussions\n      avg_open_issues_count\n      avg_is_template\n      avg_age_days\n      avg_last_commit\n      most_lda_topic\n    \n  \n  \n    \n      0\n      -1\n      5077.636364\n      0.727273\n      0.727273\n      1.000000\n      0.515152\n      0.303030\n      0.454545\n      268.060606\n      0.030303\n      1156.363636\n      38.848485\n      8\n    \n    \n      1\n      0\n      128.913835\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      0.109223\n      0.095874\n      6.394417\n      0.031553\n      535.952670\n      255.978155\n      8\n    \n    \n      2\n      1\n      251.333333\n      1.000000\n      0.939394\n      1.000000\n      0.060606\n      0.333333\n      0.363636\n      23.757576\n      0.060606\n      531.090909\n      99.515152\n      2\n    \n    \n      3\n      2\n      82.085106\n      1.000000\n      0.021277\n      0.978723\n      0.000000\n      0.170213\n      0.212766\n      7.425532\n      0.021277\n      593.021277\n      117.148936\n      8\n    \n    \n      4\n      3\n      52.166667\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      239.333333\n      147.666667\n      1\n    \n    \n      5\n      4\n      47.833333\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n      0.166667\n      0.000000\n      8.666667\n      0.000000\n      1483.833333\n      8.333333\n      9\n    \n  \n\n\n\n\n\n(numeric_df\n    >> count(_.lda_topic)\n    >> arrange(-_.n))\n\n\n\n\n\n  \n    \n      \n      lda_topic\n      n\n    \n  \n  \n    \n      7\n      7\n      106\n    \n    \n      11\n      11\n      84\n    \n    \n      0\n      0\n      72\n    \n    \n      1\n      1\n      71\n    \n    \n      2\n      2\n      61\n    \n    \n      8\n      8\n      52\n    \n    \n      10\n      10\n      49\n    \n    \n      14\n      14\n      49\n    \n    \n      19\n      19\n      49\n    \n    \n      15\n      15\n      42\n    \n    \n      17\n      17\n      39\n    \n    \n      6\n      6\n      37\n    \n    \n      13\n      13\n      37\n    \n    \n      16\n      16\n      37\n    \n    \n      18\n      18\n      34\n    \n    \n      4\n      4\n      28\n    \n    \n      5\n      5\n      28\n    \n    \n      9\n      9\n      28\n    \n    \n      12\n      12\n      28\n    \n    \n      3\n      3\n      18"
  },
  {
    "objectID": "project/02_eda.html",
    "href": "project/02_eda.html",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "",
    "text": "import pandas as pd\nfrom siuba import *\nfrom siuba.siu import call\nfrom plotnine import *\nimport json\nimport matplotlib.pyplot as pp\nScope: looking at just tools.\nHow to prove if something is an OSS tool?\nthink of removing\nWhat other tags to collect?\nOther metrics to bring in?"
  },
  {
    "objectID": "project/02_eda.html#q1-how-much-are-these-projects-growing",
    "href": "project/02_eda.html#q1-how-much-are-these-projects-growing",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Q1: How much are these projects growing?",
    "text": "Q1: How much are these projects growing?\n\ngrowth = (\n    commits_df\n    >> arrange(_.committer_date)\n    >> mutate(\n        n_1_commits=1,\n        cumsum_commits=_.groupby(\"repository_id\")[\"n_1_commits\"].transform(\n            pd.Series.cumsum\n        ),\n    )\n    >> ungroup()\n)\n\n\n# convert this to datetime2013-05-15T08:35:45+02:00\n(growth\n    >> filter(_.repository_id == _.repository_id.iloc[0])\n    >> mutate(\n        committer_date_dt = call(pd.to_datetime, _.committer_date)\n    )\n    # >> _.set_index(\"committer_date\")\n    # >> _.resample(rule='M', on='committer_date')['cumsum_commits']\n    # >> ggplot()\n    # + geom_line(aes('committer_date', 'cumsum_commits'))\n)#DateTimeIndex('committer_date_dt')#.resample(rule='M', on='committer_date')\n\n\n\n\n\n  \n    \n      \n      sha\n      repository_id\n      author_email\n      committer_email\n      author_name\n      author_date\n      committer_name\n      committer_date\n      message\n      n_1_commits\n      cumsum_commits\n      committer_date_dt\n    \n  \n  \n    \n      6095\n      3fa5994f589a6b73d1b435cada74f84ece7788f0\n      MDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\n      jdowling@kth.se\n      jdowling@kth.se\n      Jim Dowling\n      2013-05-15T08:35:45+02:00\n      Jim Dowling\n      2013-05-15T08:35:45+02:00\n      Imported KTHFS Dashboard\n      1\n      1\n      2013-05-15 08:35:45+02:00\n    \n    \n      6094\n      41f8ffb66dc1a3a8eb60e4921dbad8cac55be4d1\n      MDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\n      jdowling@kth.se\n      jdowling@kth.se\n      Jim Dowling\n      2013-05-15T08:42:00+02:00\n      Jim Dowling\n      2013-05-15T08:42:00+02:00\n      Removed script for creating jarmon symlink\n      1\n      2\n      2013-05-15 08:42:00+02:00\n    \n    \n      6093\n      51fa793432d0126f7c4d821fc4db685a84bef2bb\n      MDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\n      afzali@kth.se\n      afzali@kth.se\n      Hamidreza Afzali\n      2013-05-21T16:45:21+02:00\n      Hamidreza Afzali\n      2013-05-21T16:45:21+02:00\n      Upgraded to a newer version copied from kthfs ...\n      1\n      3\n      2013-05-21 16:45:21+02:00\n    \n    \n      6092\n      9813c228a0aa6f73a50b21c66a03c34664df63fb\n      MDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\n      jdowling@sics.se\n      jdowling@sics.se\n      Jim Dowling\n      2013-05-21T17:04:20+02:00\n      Jim Dowling\n      2013-05-21T17:04:20+02:00\n      Merge branch 'hamid_upgrade' of ghetto.sics.se...\n      1\n      4\n      2013-05-21 17:04:20+02:00\n    \n    \n      6091\n      ad3af55e5e33aea81294697273f6571089b6c4d3\n      MDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\n      a.lorenteleal@gmail.com\n      a.lorenteleal@gmail.com\n      alorlea\n      2013-05-21T16:23:04+02:00\n      alorlea\n      2013-05-21T17:20:52+02:00\n      added the async api support\n      1\n      5\n      2013-05-21 17:20:52+02:00\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4\n      50049920c9a6fa915d997be1aae15f942f89d4cc\n      MDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\n      ormenisan.adrian@gmail.com\n      noreply@github.com\n      Alex Ormenisan\n      2023-01-19T16:05:29+02:00\n      GitHub\n      2023-01-19T16:05:29+02:00\n      [HWORKS-372] explicit provenance missing commu...\n      1\n      6092\n      2023-01-19 16:05:29+02:00\n    \n    \n      3\n      801f45bb8b62d954167f760aa00d266b1f1b9101\n      MDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\n      55157590+DhananjayMukhedkar@users.noreply.gith...\n      noreply@github.com\n      Dhananjay Mukhedkar\n      2023-01-19T16:56:27+01:00\n      GitHub\n      2023-01-19T16:56:27+01:00\n      [FSTORE-313] fix null check for feature group...\n      1\n      6093\n      2023-01-19 16:56:27+01:00\n    \n    \n      2\n      8f291f3a58e07b60a8991b9ac9d870553f23106b\n      MDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\n      robin.eric.andersson@gmail.com\n      noreply@github.com\n      Robin Andersson\n      2023-01-19T16:57:39+01:00\n      GitHub\n      2023-01-19T16:57:39+01:00\n      [FSTORE-388][APPEND] Move Agent role to correc...\n      1\n      6094\n      2023-01-19 16:57:39+01:00\n    \n    \n      1\n      2a55ea7bc77e6cbef538e10e63a3bebdd7c97bd8\n      MDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\n      antonios@logicalclocks.com\n      noreply@github.com\n      Antonis Kouzoupis\n      2023-01-23T13:08:59+01:00\n      GitHub\n      2023-01-23T13:08:59+01:00\n      [HWORKS-380] Close PEMParser resource (#1288)\n      1\n      6095\n      2023-01-23 13:08:59+01:00\n    \n    \n      0\n      c0e6a69309a4f0a14f66dbb81d0ee54c0c3b7979\n      MDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\n      8422705+moritzmeister@users.noreply.github.com\n      noreply@github.com\n      Moritz Meister\n      2023-01-24T14:11:10+01:00\n      GitHub\n      2023-01-24T14:11:10+01:00\n      [FSTORE-614] Auto Kafka Topic recreation needs...\n      1\n      6096\n      2023-01-24 14:11:10+01:00\n    \n  \n\n6096 rows × 12 columns\n\n\n\nsliders for time, select per repo or per cluster (from labels?)"
  },
  {
    "objectID": "project/02_eda.html#stargazers",
    "href": "project/02_eda.html#stargazers",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Stargazers",
    "text": "Stargazers\n\nlen(star_df.user_id.unique())\n\n72664\n\n\n\nlen(star_df.repository_id.unique())\n\n5\n\n\n\ncount_authors = (\n    star_df[\"user_id\"]\n    .value_counts()\n    .value_counts()\n    .to_frame()\n    .reset_index()\n    .rename(columns={\"index\": \"n_repos_starred\", \"user_id\": \"user_count\"})\n)\n\n(ggplot(count_authors, aes(\"n_repos_starred\", \"user_count\")) + geom_point())\n\n\n\n\n<ggplot: (821561167)>\n\n\npeople tend to only star one framework is it all the same framework?\ndo the people who star more contribute more?\n\n_1 = (\n    star_df[\"user_id\"]\n    .value_counts()\n    .to_frame()\n    .reset_index()\n    .rename(columns={\"index\": \"user_id\", \"user_id\": \"n_repos_starred\"})\n)\n\n\n# just use for joins \n_2 = star_df >> full_join(_, _1, on=\"user_id\")\n_3 = repo_df[[\"full_name\", \"node_id\"]].rename(\n    columns={\"node_id\": \"repository_id\"}\n)\n\n\n## users + how many repos they star + repo name\nstars_repo_name = _2 >> left_join(_, _3, on=\"repository_id\")\nstars_repo_name\n\n\n\n\n\n  \n    \n      \n      repository_id\n      user_id\n      user_login\n      starred_at\n      n_repos_starred\n      full_name\n    \n  \n  \n    \n      0\n      MDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\n      MDQ6VXNlcjE0ODY3ODE5\n      raghuch\n      2018-11-21T11:50:55Z\n      1\n      GokuMohandas/Made-With-ML\n    \n    \n      1\n      MDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\n      MDQ6VXNlcjEzODgxMDA=\n      vishalbelsare\n      2018-11-30T14:34:05Z\n      3\n      GokuMohandas/Made-With-ML\n    \n    \n      2\n      MDEwOlJlcG9zaXRvcnkyNDAzMTUwNDY=\n      MDQ6VXNlcjEzODgxMDA=\n      vishalbelsare\n      2020-06-15T08:32:23Z\n      3\n      jina-ai/jina\n    \n    \n      3\n      MDEwOlJlcG9zaXRvcnkxMzU2NzM0NTE=\n      MDQ6VXNlcjEzODgxMDA=\n      vishalbelsare\n      2021-07-20T16:35:12Z\n      3\n      microsoft/nni\n    \n    \n      4\n      MDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\n      MDQ6VXNlcjM1NTM3MDE0\n      paulananth\n      2018-12-01T00:48:08Z\n      1\n      GokuMohandas/Made-With-ML\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      86581\n      MDEwOlJlcG9zaXRvcnkxOTI2NDA1Mjk=\n      MDQ6VXNlcjg3MTE2NTcw\n      hyunsungK\n      2023-01-25T11:17:15Z\n      1\n      heartexlabs/label-studio\n    \n    \n      86582\n      MDEwOlJlcG9zaXRvcnkxOTI2NDA1Mjk=\n      MDQ6VXNlcjcxNDM1OQ==\n      ourgit\n      2023-01-25T14:34:50Z\n      1\n      heartexlabs/label-studio\n    \n    \n      86583\n      MDEwOlJlcG9zaXRvcnkxOTI2NDA1Mjk=\n      MDQ6VXNlcjU4MzU5NjI1\n      alcompa\n      2023-01-25T16:02:09Z\n      1\n      heartexlabs/label-studio\n    \n    \n      86584\n      MDEwOlJlcG9zaXRvcnkxOTI2NDA1Mjk=\n      MDQ6VXNlcjExOTQ4MjUw\n      jugodu\n      2023-01-25T19:34:15Z\n      1\n      heartexlabs/label-studio\n    \n    \n      86585\n      MDEwOlJlcG9zaXRvcnkxOTI2NDA1Mjk=\n      MDQ6VXNlcjU0NzgzNjA5\n      zzhenyu23\n      2023-01-25T21:50:00Z\n      1\n      heartexlabs/label-studio\n    \n  \n\n86586 rows × 6 columns\n\n\n\n\nassert len(stars_repo_name.full_name.unique()) == len(star_df.repository_id.unique())"
  },
  {
    "objectID": "project/02_eda.html#what-are-these-tools-doing",
    "href": "project/02_eda.html#what-are-these-tools-doing",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "What are these tools doing?",
    "text": "What are these tools doing?\nie, monitoring, for certain model, etc\nfrom description? README? probably not commits\nlanguages from gh look at .gitattributes (in python world, dependency signals?) ^^"
  },
  {
    "objectID": "project/02_eda.html#how-active-are-these-people",
    "href": "project/02_eda.html#how-active-are-these-people",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "How active are these people?",
    "text": "How active are these people?\n\nwatchers\nusers\nbug reporters\nPRs\ncore maintainers\n\nTODO:\n\n[] pull more data for stargazers\n[] answer more questions\n[] start thinking about modeling\n[] wireframe shiny UI (likely shiny for python)\n\nthings to include for shiny:"
  },
  {
    "objectID": "project/03_feature_engineering.html",
    "href": "project/03_feature_engineering.html",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "",
    "text": "import pandas as pd\nfrom siuba import *\nfrom siuba.siu import call\nfrom plotnine import *\nimport json\nimport matplotlib.pyplot as pp\nimport tidytext\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nimport datetime\n\nimport numpy as np\n\nnp.random.seed(500)"
  },
  {
    "objectID": "project/03_feature_engineering.html#not-scaled",
    "href": "project/03_feature_engineering.html#not-scaled",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "not scaled",
    "text": "not scaled\n\npca_numeric = PCA(n_components=2).fit(numeric_df)\npca_numeric_output = pca_numeric.transform(numeric_df)\n\n\nnumeric_df.dtypes\n\nstargazers_count               int64\nhas_issues                      bool\nhas_projects                    bool\nhas_downloads                   bool\nhas_wiki                        bool\nhas_pages                       bool\nhas_discussions                 bool\nopen_issues_count              int64\nallow_forking                   bool\nis_template                     bool\nage_days                       int64\ntime_since_last_commit_days    int64\ndtype: object\n\n\n\npca_numeric.components_\n\narray([[ 9.95783126e-01,  1.90231055e-06,  1.51537777e-06,\n         1.08088645e-07, -1.47462294e-05,  2.12904286e-05,\n         3.38724621e-05,  2.26738089e-02,  0.00000000e+00,\n        -1.64106884e-06,  8.59427426e-02, -2.27092158e-02],\n       [-7.49244672e-02, -1.14259322e-05, -2.71273552e-05,\n         1.00932636e-06, -3.19559086e-05, -1.89731021e-05,\n         3.01801733e-05,  3.11311337e-02, -0.00000000e+00,\n        -1.40761332e-05,  9.44231553e-01,  3.19129985e-01]])\n\n\n\n# https://stackoverflow.com/questions/47370795/pca-on-sklearn-how-to-interpret-pca-components\n# good PCA refresher https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e\n\n\ndef pca_plot(transformed_data, components):\n\n    x = transformed_data[:, 0]\n    y = transformed_data[:, 1]\n    n = components.shape[0]\n\n    plt.scatter(x, y)\n    plt.xlabel(\"PC{}\".format(1))\n    plt.ylabel(\"PC{}\".format(2))\n    plt.grid()\n    \n    for i in range(n):\n        plt.arrow(0, 0, components[i, 0], components[i, 1], color=\"black\", alpha=0.5)\n        plt.text(\n            components[i, 0] * 1.3,\n            components[i, 1] * 1.3,\n            \"Var \" + str(i + 1),\n            color=\"black\",\n            ha=\"center\",\n            va=\"center\",\n        )\n\ndef most_important_feature(components, data):\n    most_important = [np.abs(components[i]).argmax() for i in range(components.shape[0])]\n\n    initial_feature_names = data.columns\n\n    most_important_names = [initial_feature_names[most_important[i]] for i in range(components.shape[0])]\n\n    dic = {'PC{}'.format(i): most_important_names[i] for i in range(components.shape[0])}\n\n\n    return pd.DataFrame(dic.items())\n\n\nplt.title(\"PCA of MLOps GitHub dataset, numeric columns\")\npca_plot(pca_numeric_output, pca_numeric.components_) \nplt.show()\n\n\n\n\n\nmost_important_feature(pca_numeric.components_, numeric_df)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n    \n  \n  \n    \n      0\n      PC0\n      stargazers_count\n    \n    \n      1\n      PC1\n      age_days"
  },
  {
    "objectID": "project/03_feature_engineering.html#scaled",
    "href": "project/03_feature_engineering.html#scaled",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "scaled",
    "text": "scaled\nshould i transform this data? feature scaling https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py\n\nscaled_numeric_df = StandardScaler().fit_transform(numeric_df)\npca_numeric_scaled = PCA(n_components=2).fit(scaled_numeric_df)\npca_numeric_scaled_output = pca_numeric_scaled.transform(scaled_numeric_df)\n\n\nnumeric_df.columns\n\nIndex(['stargazers_count', 'has_issues', 'has_projects', 'has_downloads',\n       'has_wiki', 'has_pages', 'has_discussions', 'open_issues_count',\n       'allow_forking', 'is_template', 'age_days',\n       'time_since_last_commit_days'],\n      dtype='object')\n\n\nwhat does this mean?\n\nplt.title(\"PCA of MLOps GitHub dataset, scaled numeric columns\")\n\npca_plot(pca_numeric_scaled_output, pca_numeric_scaled.components_) \nplt.show()\n\n\n\n\n\n# from https://scikit-learn.org/stable/auto_examples/cluster/plot_bisect_kmeans.html#sphx-glr-auto-examples-cluster-plot-bisect-kmeans-py\n \nfrom sklearn.cluster import KMeans, DBSCAN\nn_clusters_list = [3, 4, 5, 8]\n\nclustering_algorithms = {\n    \"K-Means\": KMeans,\n}\n\n# Make subplots for each variant\nfig, axs = plt.subplots(\n    len(clustering_algorithms), len(n_clusters_list), figsize=(12, 5)\n)\n\n\nfor i, (algorithm_name, Algorithm) in enumerate(clustering_algorithms.items()):\n    for j, n_clusters in enumerate(n_clusters_list):\n        algo = Algorithm(n_clusters=n_clusters, random_state=500, n_init=3)\n        algo.fit(pca_numeric_scaled_output)\n        centers = algo.cluster_centers_\n\n        axs[j].scatter(pca_numeric_scaled_output[:, 0], pca_numeric_scaled_output[:, 1], s=10, c=algo.labels_)\n        axs[j].scatter(centers[:, 0], centers[:, 1], c=\"r\", s=20)\n\n        axs[j].set_title(f\"{algorithm_name} : {n_clusters} clusters\")\n\n\n# Hide x labels and tick labels for top plots and y ticks for right plots.\nfor ax in axs.flat:\n    ax.label_outer()\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.show()\n\n\n\n\n\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics\n\ndb = DBSCAN(eps = 0.5, min_samples=5).fit(pca_numeric_scaled_output)\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint(\"Estimated number of clusters: %d\" % n_clusters_)\nprint(\"Estimated number of noise points: %d\" % n_noise_)\n\nEstimated number of clusters: 5\nEstimated number of noise points: 32\n\n\n\nunique_labels = set(labels)\ncore_samples_mask = np.zeros_like(labels, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\ncolors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = labels == k\n\n    xy = pca_numeric_scaled_output[class_member_mask & core_samples_mask]\n    plt.plot(\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=14,\n    )\n\n    xy = pca_numeric_scaled_output[class_member_mask & ~core_samples_mask]\n    plt.plot(\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\n\nplt.title(f\"DBSCAN estimated number of clusters: {n_clusters_}\")\nplt.show()\n\n\n\n\n\nnumeric_df['pca_x'] = pca_numeric_scaled_output[:,0]\nnumeric_df['pca_y'] = pca_numeric_scaled_output[:,1]\n\n\n(numeric_df\n    >> filter(_.pca_x < 0, _.pca_y < 0)\n)\n\n\n\n\n\n  \n    \n      \n      stargazers_count\n      has_issues\n      has_projects\n      has_downloads\n      has_wiki\n      has_pages\n      has_discussions\n      open_issues_count\n      allow_forking\n      is_template\n      age_days\n      time_since_last_commit_days\n      pca_x\n      pca_y\n    \n  \n  \n    \n      70\n      398\n      True\n      True\n      True\n      True\n      False\n      False\n      2\n      True\n      False\n      238\n      2\n      -0.531758\n      -0.192959\n    \n    \n      71\n      396\n      True\n      True\n      True\n      True\n      False\n      False\n      1\n      True\n      False\n      136\n      81\n      -0.664739\n      -0.274867\n    \n    \n      77\n      357\n      True\n      True\n      True\n      True\n      False\n      False\n      3\n      True\n      False\n      430\n      88\n      -0.513025\n      -0.041034\n    \n    \n      93\n      240\n      True\n      True\n      True\n      True\n      False\n      False\n      40\n      True\n      False\n      261\n      2\n      -0.398149\n      -0.008217\n    \n    \n      117\n      11\n      True\n      True\n      True\n      True\n      False\n      False\n      1\n      True\n      False\n      275\n      45\n      -0.622603\n      -0.274088\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1038\n      0\n      True\n      True\n      True\n      True\n      False\n      False\n      3\n      True\n      False\n      98\n      21\n      -0.688753\n      -0.407986\n    \n    \n      1039\n      0\n      True\n      True\n      True\n      True\n      False\n      False\n      0\n      True\n      False\n      31\n      31\n      -0.745678\n      -0.477094\n    \n    \n      1040\n      0\n      True\n      True\n      True\n      True\n      False\n      False\n      0\n      True\n      False\n      305\n      10\n      -0.579480\n      -0.261147\n    \n    \n      1041\n      0\n      True\n      True\n      True\n      True\n      False\n      False\n      0\n      True\n      False\n      231\n      4\n      -0.613402\n      -0.320219\n    \n    \n      1042\n      0\n      True\n      True\n      True\n      True\n      False\n      False\n      0\n      True\n      False\n      44\n      10\n      -0.719003\n      -0.468135\n    \n  \n\n458 rows × 14 columns\n\n\n\n\n(numeric_df\n    >> filter(_.pca_x > 1, _.pca_x < 2, _.pca_y < 0)\n)\n\n\n\n\n\n  \n    \n      \n      stargazers_count\n      has_issues\n      has_projects\n      has_downloads\n      has_wiki\n      has_pages\n      has_discussions\n      open_issues_count\n      allow_forking\n      is_template\n      age_days\n      time_since_last_commit_days\n      pca_x\n      pca_y\n    \n  \n  \n    \n      56\n      677\n      True\n      False\n      True\n      True\n      False\n      False\n      130\n      True\n      False\n      276\n      2\n      1.822870\n      -0.943839\n    \n    \n      91\n      242\n      True\n      True\n      True\n      False\n      False\n      False\n      18\n      True\n      False\n      548\n      83\n      1.258243\n      -0.812920\n    \n    \n      162\n      127\n      True\n      True\n      True\n      False\n      False\n      False\n      13\n      True\n      False\n      470\n      44\n      1.213945\n      -0.936983\n    \n    \n      196\n      79\n      True\n      True\n      True\n      False\n      True\n      False\n      0\n      True\n      False\n      11\n      9\n      1.528874\n      -0.978110\n    \n    \n      198\n      78\n      True\n      True\n      True\n      False\n      False\n      True\n      10\n      True\n      False\n      485\n      326\n      1.835465\n      -0.089116\n    \n    \n      208\n      72\n      True\n      False\n      True\n      True\n      False\n      False\n      0\n      True\n      False\n      345\n      224\n      1.039869\n      -1.763245\n    \n    \n      222\n      55\n      True\n      True\n      True\n      False\n      False\n      False\n      16\n      True\n      False\n      121\n      2\n      1.066207\n      -1.219881\n    \n    \n      227\n      54\n      True\n      True\n      True\n      False\n      False\n      False\n      5\n      True\n      False\n      378\n      8\n      1.154828\n      -1.076884\n    \n    \n      291\n      25\n      True\n      True\n      True\n      False\n      True\n      False\n      4\n      True\n      False\n      668\n      379\n      1.538900\n      -0.426156\n    \n    \n      299\n      23\n      True\n      True\n      True\n      False\n      False\n      False\n      72\n      True\n      False\n      1246\n      2\n      1.880799\n      -0.026428\n    \n    \n      314\n      20\n      True\n      True\n      True\n      False\n      False\n      False\n      18\n      True\n      False\n      723\n      93\n      1.304350\n      -0.735272\n    \n    \n      321\n      19\n      True\n      True\n      True\n      False\n      False\n      False\n      20\n      True\n      False\n      290\n      5\n      1.163181\n      -1.073525\n    \n    \n      363\n      13\n      True\n      True\n      True\n      False\n      True\n      False\n      11\n      True\n      False\n      538\n      5\n      1.845995\n      -0.517867\n    \n    \n      369\n      12\n      True\n      True\n      True\n      False\n      True\n      False\n      5\n      True\n      False\n      616\n      198\n      1.682798\n      -0.477111\n    \n    \n      395\n      10\n      True\n      True\n      True\n      False\n      True\n      False\n      0\n      True\n      False\n      768\n      32\n      1.900114\n      -0.395489\n    \n    \n      424\n      7\n      True\n      True\n      True\n      False\n      True\n      False\n      0\n      True\n      False\n      39\n      16\n      1.524927\n      -0.975491\n    \n    \n      509\n      4\n      False\n      True\n      True\n      True\n      False\n      False\n      0\n      True\n      False\n      300\n      138\n      1.308122\n      -2.643825\n    \n    \n      580\n      3\n      False\n      True\n      True\n      True\n      False\n      False\n      0\n      True\n      False\n      240\n      220\n      1.198851\n      -2.686414\n    \n    \n      655\n      2\n      True\n      True\n      True\n      False\n      True\n      False\n      0\n      True\n      False\n      8\n      4\n      1.518770\n      -1.002239\n    \n    \n      675\n      1\n      True\n      False\n      True\n      True\n      True\n      False\n      0\n      True\n      False\n      203\n      187\n      1.572961\n      -1.487499\n    \n    \n      767\n      1\n      False\n      True\n      True\n      True\n      False\n      False\n      0\n      True\n      False\n      306\n      94\n      1.352146\n      -2.642730\n    \n    \n      878\n      0\n      True\n      True\n      True\n      False\n      True\n      False\n      1\n      True\n      False\n      450\n      177\n      1.596108\n      -0.635603\n    \n    \n      926\n      0\n      True\n      False\n      True\n      True\n      False\n      False\n      0\n      True\n      False\n      632\n      186\n      1.216645\n      -1.558117\n    \n    \n      1043\n      0\n      True\n      True\n      True\n      False\n      False\n      False\n      0\n      True\n      True\n      984\n      3\n      1.459667\n      -0.790121\n    \n  \n\n\n\n\n\nmost_important_feature(pca_numeric_scaled.components_, numeric_df)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n    \n  \n  \n    \n      0\n      PC0\n      has_wiki\n    \n    \n      1\n      PC1\n      open_issues_count"
  },
  {
    "objectID": "project/03_feature_engineering.html#what-about-the-self-labeled-topics-at-the-repo-level",
    "href": "project/03_feature_engineering.html#what-about-the-self-labeled-topics-at-the-repo-level",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "what about the self labeled topics at the repo level?",
    "text": "what about the self labeled topics at the repo level?\n\nrefined_df.topics.iloc[0]\n\n['data-engineering',\n 'data-science',\n 'deep-learning',\n 'machine-learning',\n 'mlops',\n 'natural-language-processing',\n 'python',\n 'pytorch']\n\n\n\nfrom collections import Counter\n\nall_topics = (\n    pd.DataFrame.from_dict(Counter(refined_df.topics.sum()), orient=\"index\")\n    .reset_index()\n    .rename(columns={\"index\": \"topics\", 0: \"n\"})\n)\n\n\n(all_topics\n    >> arrange(-_.n)\n)\n\n\n\n\n\n  \n    \n      \n      topics\n      n\n    \n  \n  \n    \n      4\n      mlops\n      938\n    \n    \n      3\n      machine-learning\n      512\n    \n    \n      6\n      python\n      237\n    \n    \n      1\n      data-science\n      195\n    \n    \n      2\n      deep-learning\n      169\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      1739\n      full-stack\n      1\n    \n    \n      1740\n      plant\n      1\n    \n    \n      1741\n      plantdiseasedetection\n      1\n    \n    \n      1742\n      web\n      1\n    \n    \n      1743\n      azuremlops\n      1\n    \n  \n\n1744 rows × 2 columns\n\n\n\n\n(all_topics\n    >> filter(_.n > 1)\n    >> count()\n)\n\n\n\n\n\n  \n    \n      \n      n\n    \n  \n  \n    \n      0\n      610\n    \n  \n\n\n\n\n\nall_topics['topics'] = all_topics.topics.str.replace(\"-\", \" \")\nall_topics\n\n\n\n\n\n  \n    \n      \n      topics\n      n\n    \n  \n  \n    \n      0\n      data engineering\n      30\n    \n    \n      1\n      data science\n      195\n    \n    \n      2\n      deep learning\n      169\n    \n    \n      3\n      machine learning\n      512\n    \n    \n      4\n      mlops\n      938\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      1739\n      full stack\n      1\n    \n    \n      1740\n      plant\n      1\n    \n    \n      1741\n      plantdiseasedetection\n      1\n    \n    \n      1742\n      web\n      1\n    \n    \n      1743\n      azuremlops\n      1\n    \n  \n\n1744 rows × 2 columns"
  },
  {
    "objectID": "project/03_feature_engineering.html#what-about-licenses",
    "href": "project/03_feature_engineering.html#what-about-licenses",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "what about licenses?",
    "text": "what about licenses?\n\nrefined_df['license'] = refined_df.license.apply(pd.Series, dtype = 'object').key\n\n\nrefined_df.license.astype(\"category\")\n\n0              mit\n1       apache-2.0\n2              mit\n3              mit\n4       apache-2.0\n           ...    \n1039           NaN\n1040           NaN\n1041           NaN\n1042           mit\n1043           mit\nName: license, Length: 986, dtype: category\nCategories (13, object): ['agpl-3.0', 'apache-2.0', 'bsd-2-clause', 'bsd-3-clause', ..., 'mit', 'mit-0', 'mpl-2.0', 'other']\n\n\n\nrefined_df\n\n\n\n\n\n  \n    \n      \n      id\n      node_id\n      name\n      full_name\n      owner\n      description\n      created_at\n      updated_at\n      pushed_at\n      stargazers_count\n      ...\n      has_wiki\n      has_pages\n      has_discussions\n      open_issues_count\n      license\n      allow_forking\n      is_template\n      topics\n      age_days\n      time_since_last_commit_days\n    \n  \n  \n    \n      0\n      156157055\n      MDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\n      Made-With-ML\n      GokuMohandas/Made-With-ML\n      {'login': 'GokuMohandas', 'id': 8000987, 'node...\n      Learn how to responsibly develop, deploy and m...\n      2018-11-05 03:44:27\n      2023-01-18T14:24:54Z\n      2022-11-08 13:52:44\n      32107\n      ...\n      True\n      False\n      False\n      6\n      mit\n      True\n      False\n      [data-engineering, data-science, deep-learning...\n      1537\n      73\n    \n    \n      1\n      240315046\n      MDEwOlJlcG9zaXRvcnkyNDAzMTUwNDY=\n      jina\n      jina-ai/jina\n      {'login': 'jina-ai', 'id': 60539444, 'node_id'...\n      🔮 Build multimodal AI services via cloud nativ...\n      2020-02-13 17:04:44\n      2023-01-18T14:18:14Z\n      2023-01-18 16:21:18\n      17144\n      ...\n      False\n      True\n      False\n      33\n      apache-2.0\n      True\n      False\n      [aiops, airflow, cloud-native, creative-ai, cr...\n      1072\n      2\n    \n    \n      2\n      144863525\n      MDEwOlJlcG9zaXRvcnkxNDQ4NjM1MjU=\n      awesome-production-machine-learning\n      EthicalML/awesome-production-machine-learning\n      {'login': 'EthicalML', 'id': 43532924, 'node_i...\n      A curated list of awesome open source librarie...\n      2018-08-15 14:28:41\n      2023-01-18T17:16:27Z\n      2023-01-16 06:42:25\n      13008\n      ...\n      True\n      True\n      False\n      26\n      mit\n      True\n      False\n      [awesome, awesome-list, data-mining, deep-lear...\n      1619\n      4\n    \n    \n      3\n      135673451\n      MDEwOlJlcG9zaXRvcnkxMzU2NzM0NTE=\n      nni\n      microsoft/nni\n      {'login': 'microsoft', 'id': 6154722, 'node_id...\n      An open source AutoML toolkit for automate mac...\n      2018-06-01 05:51:44\n      2023-01-18T12:49:05Z\n      2023-01-18 09:27:55\n      12415\n      ...\n      True\n      False\n      True\n      290\n      mit\n      True\n      False\n      [automated-machine-learning, automl, bayesian-...\n      1694\n      2\n    \n    \n      4\n      192640529\n      MDEwOlJlcG9zaXRvcnkxOTI2NDA1Mjk=\n      label-studio\n      heartexlabs/label-studio\n      {'login': 'heartexlabs', 'id': 48309720, 'node...\n      Label Studio is a multi-type data labeling and...\n      2019-06-19 02:00:44\n      2023-01-18T11:31:18Z\n      2023-01-18 17:09:25\n      11747\n      ...\n      True\n      False\n      True\n      448\n      apache-2.0\n      True\n      False\n      [annotation, annotation-tool, annotations, bou...\n      1311\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1039\n      580399205\n      R_kgDOIpgwZQ\n      step-functions-sagemaker-ml-pipeline\n      hkford/step-functions-sagemaker-ml-pipeline\n      {'login': 'hkford', 'id': 82389275, 'node_id':...\n      ML pipeline with AWS Step Functions and Amazon...\n      2022-12-20 13:17:41\n      2022-12-20T13:33:21Z\n      2022-12-20 13:32:14\n      0\n      ...\n      True\n      False\n      False\n      0\n      NaN\n      True\n      False\n      [mlops, sagemaker, step-functions]\n      31\n      31\n    \n    \n      1040\n      472158586\n      R_kgDOHCSReg\n      genome\n      edeliu2000/genome\n      {'login': 'edeliu2000', 'id': 8235794, 'node_i...\n      AI AutoML Platform Services for Managing Milli...\n      2022-03-21 02:18:15\n      2022-12-09T17:27:37Z\n      2023-01-10 23:03:03\n      0\n      ...\n      True\n      False\n      False\n      0\n      NaN\n      True\n      False\n      [automlops, configurable-ai, controllable-ai, ...\n      305\n      10\n    \n    \n      1041\n      499422453\n      R_kgDOHcSU9Q\n      ml-ops\n      khoaguin/ml-ops\n      {'login': 'khoaguin', 'id': 88959106, 'node_id...\n      A Learning Journal on Machine Learning in Prod...\n      2022-06-03 07:38:42\n      2022-06-10T08:40:17Z\n      2023-01-16 07:24:02\n      0\n      ...\n      True\n      False\n      False\n      0\n      NaN\n      True\n      False\n      [mlops]\n      231\n      4\n    \n    \n      1042\n      575407571\n      R_kgDOIkwF0w\n      Plant-disease-Detection\n      shashank1623/Plant-disease-Detection\n      {'login': 'shashank1623', 'id': 86946068, 'nod...\n      Plant Disease Detection  using convolutional n...\n      2022-12-07 12:53:33\n      2023-01-10T17:25:44Z\n      2023-01-10 17:29:52\n      0\n      ...\n      True\n      False\n      False\n      0\n      mit\n      True\n      False\n      [adam-optimizer, cnn, d, deep-learning, develo...\n      44\n      10\n    \n    \n      1043\n      262970855\n      MDEwOlJlcG9zaXRvcnkyNjI5NzA4NTU=\n      aml-templates\n      weekoflearning/aml-templates\n      {'login': 'weekoflearning', 'id': 63700392, 'n...\n      template repository for azure machine learnings\n      2020-05-11 07:30:45\n      2023-01-17T05:37:09Z\n      2023-01-17 05:37:05\n      0\n      ...\n      False\n      False\n      False\n      0\n      mit\n      True\n      True\n      [azure-ml, azuremachinelearning, azuremlops, m...\n      984\n      3\n    \n  \n\n986 rows × 24 columns"
  },
  {
    "objectID": "project/03_feature_engineering.html#what-do-we-know-about-the-prs",
    "href": "project/03_feature_engineering.html#what-do-we-know-about-the-prs",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "what do we know about the PRs?",
    "text": "what do we know about the PRs?\n\nprs_df = pd.DataFrame(issuespr_total)\nprs_df.head()\n\n\n\n\n\n  \n    \n      \n      id\n      milestone_id\n      repository_id\n      user_id\n      body\n      closed_at\n      created_at\n      locked\n      number\n      state\n      title\n      updated_at\n      type\n    \n  \n  \n    \n      0\n      MDExOlB1bGxSZXF1ZXN0MjI4NDQ0MTI4\n      None\n      MDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\n      MDQ6VXNlcjgwMDA5ODc=\n      \n      2018-11-05T18:20:05Z\n      2018-11-05T18:14:37Z\n      False\n      1\n      MERGED\n      added instructions for running and contributin...\n      2018-11-05T18:20:05Z\n      PullRequest\n    \n    \n      1\n      MDExOlB1bGxSZXF1ZXN0MjI4NDQ2MDM3\n      None\n      MDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\n      MDQ6VXNlcjgwMDA5ODc=\n      \n      2018-11-05T18:21:46Z\n      2018-11-05T18:21:38Z\n      False\n      2\n      MERGED\n      fixed upload image link\n      2018-11-05T18:21:46Z\n      PullRequest\n    \n    \n      2\n      MDExOlB1bGxSZXF1ZXN0MjI4NDU3Mjg4\n      None\n      MDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\n      MDQ6VXNlcjgwMDA5ODc=\n      \n      2018-11-05T19:02:46Z\n      2018-11-05T19:02:25Z\n      False\n      3\n      MERGED\n      made readme more readable by adding linebreaks\n      2018-11-05T19:02:46Z\n      PullRequest\n    \n    \n      3\n      MDExOlB1bGxSZXF1ZXN0MjI4NDU3ODMy\n      None\n      MDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\n      MDQ6VXNlcjgwMDA5ODc=\n      \n      2018-11-05T19:04:31Z\n      2018-11-05T19:04:24Z\n      False\n      4\n      MERGED\n      fixed contribution instructions\n      2018-11-05T19:04:32Z\n      PullRequest\n    \n    \n      4\n      MDExOlB1bGxSZXF1ZXN0MjI4NDU4NTc0\n      None\n      MDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\n      MDQ6VXNlcjgwMDA5ODc=\n      \n      2018-11-05T19:07:24Z\n      2018-11-05T19:07:17Z\n      False\n      5\n      MERGED\n      fixed contributing instructions\n      2018-11-05T20:47:30Z\n      PullRequest\n    \n  \n\n\n\n\n\n(prs_df\n    >> distinct(_.type))\n\n\n\n\n\n  \n    \n      \n      type\n    \n  \n  \n    \n      0\n      PullRequest\n    \n  \n\n\n\n\n\n(prs_df\n    >> count(_.repository_id)\n    >> arrange(-_.n))\n\n\n\n\n\n  \n    \n      \n      repository_id\n      n\n    \n  \n  \n    \n      19\n      MDEwOlJlcG9zaXRvcnkxMzE2MTk2NDY=\n      6589\n    \n    \n      36\n      MDEwOlJlcG9zaXRvcnkxNzI4MjIxOTU=\n      6478\n    \n    \n      21\n      MDEwOlJlcG9zaXRvcnkxMzMxMDA4ODA=\n      5604\n    \n    \n      90\n      MDEwOlJlcG9zaXRvcnkyNTM4NDY4Nzk=\n      5572\n    \n    \n      11\n      MDEwOlJlcG9zaXRvcnkxMDMwNzE1MjA=\n      5320\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      267\n      R_kgDOGxQ4Bg\n      1\n    \n    \n      286\n      R_kgDOHJgXCw\n      1\n    \n    \n      293\n      R_kgDOHTLodQ\n      1\n    \n    \n      305\n      R_kgDOHhZcuw\n      1\n    \n    \n      330\n      R_kgDOIu169Q\n      1\n    \n  \n\n333 rows × 2 columns\n\n\n\n\nusers_issues = issues_df >> select(_.user_id) >> distinct()\nusers_prs = prs_df >> select(_.user_id) >> distinct()\n\nprint(f'issues: {len(users_issues)}')\nprint(f'prs: {len(users_prs)}')\n\nissues: 17649\nprs: 7373\n\n\n\n(pd.concat([users_issues, users_prs])\n    >> distinct()\n)\n\n\n\n\n\n  \n    \n      \n      user_id\n    \n  \n  \n    \n      0\n      MDQ6VXNlcjExNTMwMjQ3\n    \n    \n      1\n      MDQ6VXNlcjIyOTU5MDM3\n    \n    \n      2\n      MDQ6VXNlcjI3MTQzMDU4\n    \n    \n      3\n      MDQ6VXNlcjcwMDYxNw==\n    \n    \n      4\n      MDQ6VXNlcjExNTU1NzM=\n    \n    \n      ...\n      ...\n    \n    \n      17644\n      MDQ6VXNlcjM3NTczNjI5\n    \n    \n      17645\n      U_kgDOBt1LjA\n    \n    \n      17646\n      U_kgDOBZf1Ng\n    \n    \n      17647\n      MDQ6VXNlcjY0NDg3MDM4\n    \n    \n      17648\n      MDQ6VXNlcjEwNTM2MjE4\n    \n  \n\n17649 rows × 1 columns\n\n\n\n\nassert len(prs_df.columns) == len(issues_df.columns)\n\n\ncombine_issues_prs = pd.concat([issues_df, prs_df])\n\n\n(combine_issues_prs\n    >> distinct(_.repository_id)\n)\n\n\n\n\n\n  \n    \n      \n      repository_id\n    \n  \n  \n    \n      0\n      MDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\n    \n    \n      1\n      MDEwOlJlcG9zaXRvcnkyNDAzMTUwNDY=\n    \n    \n      2\n      MDEwOlJlcG9zaXRvcnkxNDQ4NjM1MjU=\n    \n    \n      3\n      MDEwOlJlcG9zaXRvcnkxMzU2NzM0NTE=\n    \n    \n      4\n      MDEwOlJlcG9zaXRvcnkxOTI2NDA1Mjk=\n    \n    \n      ...\n      ...\n    \n    \n      358\n      R_kgDOHj3zDw\n    \n    \n      359\n      MDEwOlJlcG9zaXRvcnk0MDIzMDY2NzE=\n    \n    \n      360\n      R_kgDOHozdZg\n    \n    \n      361\n      R_kgDOH9bggw\n    \n    \n      362\n      R_kgDOHHCVKA\n    \n  \n\n363 rows × 1 columns\n\n\n\n\nfrom siuba.dply.vector import n_distinct\n(prs_df\n    >> select(_.repository_id, _.user_id)\n    >> distinct()\n    >> group_by(_.repository_id)\n    >> mutate(n_pr_makers = n_distinct(_.user_id))\n    >> ungroup()\n    >> select(_.repository_id, _.n_pr_makers)\n    >> distinct()\n    >> arrange(-_.n_pr_makers)\n)\n\n\n\n\n\n  \n    \n      \n      repository_id\n      n_pr_makers\n    \n  \n  \n    \n      85\n      MDEwOlJlcG9zaXRvcnkxMzYyMDI2OTU=\n      725\n    \n    \n      8\n      MDEwOlJlcG9zaXRvcnkxMDc5Mzc4MTU=\n      565\n    \n    \n      23\n      MDEwOlJlcG9zaXRvcnkxMzMxMDA4ODA=\n      434\n    \n    \n      7\n      MDEwOlJlcG9zaXRvcnkxMDMwNzE1MjA=\n      398\n    \n    \n      11\n      MDEwOlJlcG9zaXRvcnkxMzE2MTk2NDY=\n      303\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      327\n      R_kgDOHJgXCw\n      1\n    \n    \n      328\n      R_kgDOHozdZg\n      1\n    \n    \n      329\n      R_kgDOH9bggw\n      1\n    \n    \n      331\n      MDEwOlJlcG9zaXRvcnkzODk0MzQwOTA=\n      1\n    \n    \n      332\n      R_kgDOIOHC-A\n      1\n    \n  \n\n333 rows × 2 columns\n\n\n\n\nlen(prs_df\n    >> distinct(_.repository_id)\n)\n\n333\n\n\n\nprs_df['created_at'] = prs_df.created_at.astype(\"datetime64[ns]\")\n# MLFLOW\n# (prs_df\n#     >> mutate(\n#         month = _.created_at.dt.month,\n#         year = _.created_at.dt.year\n#     )\n#     >> group_by(_.repository_id, _.month, _.year)\n#     >> count(_.month, _.year)\n#     >> filter(_.repository_id == 'MDEwOlJlcG9zaXRvcnkxMzYyMDI2OTU=')\n#     >> arrange(_.year, _.month)\n# )\n\n\n(prs_df\n    >> mutate(\n        year_month = _.created_at.dt.strftime('%Y-%m')\n    )\n    >> filter(_.year_month < '2023-01')\n    >> count(_.year_month)\n    >> ggplot(aes(x = 'year_month', y = 'n'))\n        + geom_point()\n        + labs(x='Date', y='Number of pull requests')\n        + scale_x_date(date_breaks = '1 year')\n        + theme_matplotlib()\n        + theme(\n            figure_size=(10,5),\n            axis_text_x=element_text(margin={'t': 5, 'r': 5})\n        )\n        + labs(\n            title=\"Gross product pull requests for all repositories\"\n        )\n)\n\n\n\n\n<ggplot: (850556356)>\n\n\n\nrefined_df >> filter(_.full_name == 'mlflow/mlflow')\n\n\n\n\n\n  \n    \n      \n      id\n      node_id\n      name\n      full_name\n      owner\n      description\n      created_at\n      updated_at\n      pushed_at\n      stargazers_count\n      ...\n      has_wiki\n      has_pages\n      has_discussions\n      open_issues_count\n      license\n      allow_forking\n      is_template\n      topics\n      age_days\n      time_since_last_commit_days\n    \n  \n  \n    \n      100\n      136202695\n      MDEwOlJlcG9zaXRvcnkxMzYyMDI2OTU=\n      mlflow\n      mlflow/mlflow\n      {'login': 'mlflow', 'id': 39938107, 'node_id':...\n      Open source platform for the machine learning ...\n      2018-06-05 16:05:58\n      2023-01-18T18:35:07Z\n      2023-01-19 00:47:53\n      13455\n      ...\n      True\n      False\n      True\n      1008\n      apache-2.0\n      True\n      False\n      [ai, apache-spark, machine-learning, ml, mlflo...\n      1690\n      1\n    \n  \n\n1 rows × 24 columns\n\n\n\n\nlen(prs_df)\n\n121704\n\n\n\n(prs_df  \n    >> filter(_.state == 'MERGED')\n    >> count()\n)\n\n\n\n\n\n  \n    \n      \n      n\n    \n  \n  \n    \n      0\n      100836\n    \n  \n\n\n\n\n\nprs_small = (prs_df\n    >> filter(_.state == 'MERGED')\n    >> select(_.repository_id, _.title)\n)\n\n\nfrom sklearn.feature_extraction import text\n\nadditional_stopwords = frozenset({\n    \"readme\",\n    \"md\",\n    \"update\",\n    \"fix\",\n    \"fixing\",\n    \"fixes\",\n    \"docs\",\n    \"add\",\n    \"updated\",\n    \"feat\",\n    \"page\",\n    \"typo\",\n    \"ci\",\n    \"cd\",\n    \"github\",\n    \"chore\",\n    \"tests\",\n    \"test\",\n    \"feature\",\n    \"updates\",\n    \"links\",\n    \"merge\",\n    \"dev\",\n    \"broken\",\n    \"example\",\n    \"cli\",\n    \"update\",\n    \"deps\",\n    \"migration\",\n    \"lint\",\n    \"setup\",\n    \"link\",\n    \"update\",\n    \"error\",\n    \"message\",\n    \"remove\",\n    \"deps\",\n    \"update\",\n    \"dependency\",\n    \"dependencies\",\n    \"requirements\",\n    \"chore\",\n    \"v1\",\n    \"components\",\n    \"merge\",\n    \"0\",\n    \"1\",\n    \"bump\",\n    \"2\",\n    \"3\",\n    \"release\",\n    \"version\",\n    \"v0\",\n    \"build\",\n    \"support\",\n    \"refactor\",\n    \"type\",\n    \"types\",\n    \"coverage\",\n    \"master\",\n    \"main\",\n    \"branch\",\n    \"clean\",\n    \"documentation\",\n    \"testing\"\n})\n\nstop_words = text.ENGLISH_STOP_WORDS.union(additional_stopwords)\n\n\n# copied from 03_clean_text.ipynb\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\ntokenizer = RegexpTokenizer(r'\\w+')\n\npr_sents  = prs_small.title.to_list()\n# Vectorize document using TF-IDF\npr_tfidf = TfidfVectorizer(lowercase=True,\n                        stop_words=stop_words,\n                        ngram_range = (1,1),\n                        tokenizer = tokenizer.tokenize)\n\n# Fit and Transform the documents\npr_train_data = pr_tfidf.fit_transform(pr_sents)   \n# Define the number of topics or components\nnum_components=5\n\n# Create LDA object\npr_model=LatentDirichletAllocation(n_components=num_components)\n\n# Fit and Transform SVD model on data\npr_lda_matrix = pr_model.fit_transform(pr_train_data)\n\n# Get Components \npr_lda_components=pr_model.components_\n\n\n# Print the topics with their terms\npr_terms = pr_tfidf.get_feature_names_out()\n\nfor index, component in enumerate(pr_lda_components):\n    zipped = zip(pr_terms, component)\n    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:7]\n    top_terms_list=list(dict(top_terms_key).keys())\n    print(\"Topic \"+str(index)+\": \",top_terms_list)\n\nTopic 0:  ['7', 'doc', 'python', 'examples', 'actions', 'maintenance', 'model']\nTopic 1:  ['5', '4', '6', 'docker', '9', 'commit', 'sdk']\nTopic 2:  ['eslint', 'plugin', 'cleanup', 'monorepo', 'spark', 'v4', 'v5']\nTopic 3:  ['changelog', 'mlflow', 'helm', 'sample', 'small', 'added', 'use']\nTopic 4:  ['api', 'run', 'make', 'integration', 'dagit', 'model', 'sdk']\n\n\n\nimport matplotlib.pyplot as plt\n\ndef plot_top_words(model, feature_names, n_top_words, title):\n    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n    axes = axes.flatten()\n    for topic_idx, topic in enumerate(model.components_):\n        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n        top_features = [feature_names[i] for i in top_features_ind]\n        weights = topic[top_features_ind]\n\n        ax = axes[topic_idx]\n        ax.barh(top_features, weights, height=0.7)\n        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n        ax.invert_yaxis()\n        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n        for i in \"top right left\".split():\n            ax.spines[i].set_visible(False)\n        fig.suptitle(title, fontsize=40)\n\n    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n    plt.show()\n\nplot_top_words(pr_model, pr_tfidf.get_feature_names_out(), 20, \"top 10 topics\")\n\n\n\n\nokay, so all the topics are around maintainance, not necessarily features or mlops-y tasks. this is pretty expected"
  },
  {
    "objectID": "project/03_feature_engineering.html#what-about-issues",
    "href": "project/03_feature_engineering.html#what-about-issues",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "what about issues?",
    "text": "what about issues?\nlooking at issues titles, not body\n\nissues_df = pd.DataFrame(issues_total)\nissues_df.head()\n\n\n\n\n\n  \n    \n      \n      id\n      milestone_id\n      repository_id\n      user_id\n      body\n      closed_at\n      created_at\n      locked\n      number\n      state\n      title\n      updated_at\n      type\n    \n  \n  \n    \n      0\n      MDU6SXNzdWUzODk2NzE0MDM=\n      None\n      MDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\n      MDQ6VXNlcjExNTMwMjQ3\n      Great repo!\\r\\nCan i translate it to Chinese?\n      2018-12-11T11:14:53Z\n      2018-12-11T09:37:03Z\n      False\n      69\n      CLOSED\n      Chinese translation\n      2022-09-16T08:27:39Z\n      Issue\n    \n    \n      1\n      MDU6SXNzdWUzOTA0OTEwMzg=\n      None\n      MDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\n      MDQ6VXNlcjIyOTU5MDM3\n      Is it okay to contribute to segmentation part ...\n      2018-12-13T16:22:16Z\n      2018-12-13T02:25:32Z\n      False\n      76\n      CLOSED\n      Contribution to Computer Vision?\n      2018-12-13T16:22:16Z\n      Issue\n    \n    \n      2\n      MDU6SXNzdWUzOTA1NDQ4OTM=\n      None\n      MDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\n      MDQ6VXNlcjI3MTQzMDU4\n      Could u pls release a instruction on Jupyter n...\n      2018-12-13T13:33:46Z\n      2018-12-13T07:05:19Z\n      False\n      78\n      CLOSED\n      Notebook\n      2018-12-13T13:33:46Z\n      Issue\n    \n    \n      3\n      MDU6SXNzdWUzOTA3NDQ3OTQ=\n      None\n      MDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\n      MDQ6VXNlcjcwMDYxNw==\n      In the [numpy notebook](https://github.com/Gok...\n      2018-12-13T16:22:02Z\n      2018-12-13T16:00:06Z\n      False\n      83\n      CLOSED\n      3d or 2d numpy array?\n      2018-12-13T16:22:02Z\n      Issue\n    \n    \n      4\n      MDU6SXNzdWUzOTI3NTY3OTc=\n      None\n      MDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\n      MDQ6VXNlcjExNTU1NzM=\n      Hi @GokuMohandas,\\r\\n\\r\\nI've been recently ta...\n      2018-12-25T20:42:17Z\n      2018-12-19T20:02:14Z\n      False\n      90\n      CLOSED\n      Alternative to Colab and Binder for running `p...\n      2018-12-26T15:44:03Z\n      Issue\n    \n  \n\n\n\n\n\nlen(issues_df)\n\n55994\n\n\n\nissues_sents  = issues_df.title.to_list()\n# Vectorize document using TF-IDF\nissues_tfidf = TfidfVectorizer(lowercase=True,\n                        stop_words=stop_words,\n                        ngram_range = (1,1),\n                        tokenizer = tokenizer.tokenize)\n\n# Fit and Transform the documents\nissues_train_data = issues_tfidf.fit_transform(pr_sents)   \n# Define the number of topics or components\nnum_components=5\n\n# Create LDA object\nissues_model=LatentDirichletAllocation(n_components=num_components)\n\n# Fit and Transform SVD model on data\nissues_lda_matrix = issues_model.fit_transform(pr_train_data)\n\n# Get Components \nissues_lda_components = issues_model.components_\n\n\n# Print the topics with their terms\nissues_terms = issues_tfidf.get_feature_names_out()\n\nfor index, component in enumerate(issues_lda_components):\n    zipped = zip(issues_terms, component)\n    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:7]\n    top_terms_list=list(dict(top_terms_key).keys())\n    print(\"Topic \"+str(index)+\": \",top_terms_list)\n\nTopic 0:  ['7', 'commit', 'pre', 'pytorch', 'sdk', 'examples', 'helm']\nTopic 1:  ['pipeline', 'api', 'det', 'use', 'sdk', 'node', 'default']\nTopic 2:  ['changelog', 'react', 'minor', 'plugin', 'eslint', 'v3', 'module']\nTopic 3:  ['4', '5', '6', 'python', '8', 'docker', 'monorepo']\nTopic 4:  ['doc', 'run', 'config', 'ui', 'api', 'store', 'command']\n\n\n\nfrom mizani.breaks import date_breaks\nfrom mizani.formatters import date_format\n(repo_df\n    >> select(_.created_at)\n    >> mutate(\n        created_at = _.created_at.astype(\"datetime64[ns]\"),\n        n = 1\n    )\n    >> arrange(_.created_at)\n    >> mutate(\n        n_cumsum = _.n.cumsum()\n    )\n    >> ggplot()\n        + geom_line(aes('created_at', 'n_cumsum'))\n        + scale_x_datetime(breaks=date_breaks('1 year'), labels=date_format('%Y'))     # modified\n        + labs(\n            x = 'Date',\n            y= 'Number of repositories',\n            title = 'Number of repositories with MLOps labels')\n        + theme_minimal()\n\n)\n\n\n\n\n<ggplot: (874157421)>"
  },
  {
    "objectID": "project/00_scrape_data.html",
    "href": "project/00_scrape_data.html",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "",
    "text": "import json\n\ndef make_file_list() -> list:\n    files = list()\n    i = 1\n    while i < 11:\n        files.append('../data/repos/topic_mlops_pg_'+str(i)+'.json')\n        if i < 2:\n            files.append('../data/repos/topic_modelmanagement_pg_'+str(i)+'.json')\n        i += 1\n    return files\n\ndef merge_files(files: list):\n    result = list()\n    for file in files:\n        with open(file, 'r') as input_file:\n            result.extend(json.load(input_file).get(\"items\"))\n\n    with open('../data/repos/topic_combined.json', 'w') as output_file:\n        json.dump(result, output_file)\n\nmerge_files(make_file_list())\nwith open(\"../data/repos/topic_combined.json\", \"r\") as read_file:\n    raw = json.load(read_file)\n\n\nassert len(raw) == 1044\n\n\nimport pandas as pd\nfrom siuba import *\n\ndf = pd.DataFrame(raw)\n\n\n(df\n    >> select(-_[\"url\":\"deployments_url\"])\n    >> filter(_.fork == False, _.private == False, _.open_issues > 0, _.license is not None)\n    >> separate(_.full_name, [\"org_name\", \"repo_name\"], sep='/')\n)\n\n\n\n\n\n  \n    \n      \n      id\n      node_id\n      name\n      private\n      owner\n      html_url\n      description\n      fork\n      created_at\n      updated_at\n      ...\n      topics\n      visibility\n      forks\n      open_issues\n      watchers\n      default_branch\n      permissions\n      score\n      org_name\n      repo_name\n    \n  \n  \n    \n      0\n      156157055\n      MDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\n      Made-With-ML\n      False\n      {'login': 'GokuMohandas', 'id': 8000987, 'node...\n      https://github.com/GokuMohandas/Made-With-ML\n      Learn how to responsibly develop, deploy and m...\n      False\n      2018-11-05T03:44:27Z\n      2023-01-18T14:24:54Z\n      ...\n      [data-engineering, data-science, deep-learning...\n      public\n      5308\n      6\n      32107\n      main\n      {'admin': False, 'maintain': False, 'push': Fa...\n      1.0\n      GokuMohandas\n      Made-With-ML\n    \n    \n      1\n      240315046\n      MDEwOlJlcG9zaXRvcnkyNDAzMTUwNDY=\n      jina\n      False\n      {'login': 'jina-ai', 'id': 60539444, 'node_id'...\n      https://github.com/jina-ai/jina\n      🔮 Build multimodal AI services via cloud nativ...\n      False\n      2020-02-13T17:04:44Z\n      2023-01-18T14:18:14Z\n      ...\n      [aiops, airflow, cloud-native, creative-ai, cr...\n      public\n      2012\n      33\n      17144\n      master\n      {'admin': False, 'maintain': False, 'push': Fa...\n      1.0\n      jina-ai\n      jina\n    \n    \n      2\n      144863525\n      MDEwOlJlcG9zaXRvcnkxNDQ4NjM1MjU=\n      awesome-production-machine-learning\n      False\n      {'login': 'EthicalML', 'id': 43532924, 'node_i...\n      https://github.com/EthicalML/awesome-productio...\n      A curated list of awesome open source librarie...\n      False\n      2018-08-15T14:28:41Z\n      2023-01-18T17:16:27Z\n      ...\n      [awesome, awesome-list, data-mining, deep-lear...\n      public\n      1817\n      26\n      13008\n      master\n      {'admin': False, 'maintain': False, 'push': Fa...\n      1.0\n      EthicalML\n      awesome-production-machine-learning\n    \n    \n      3\n      135673451\n      MDEwOlJlcG9zaXRvcnkxMzU2NzM0NTE=\n      nni\n      False\n      {'login': 'microsoft', 'id': 6154722, 'node_id...\n      https://github.com/microsoft/nni\n      An open source AutoML toolkit for automate mac...\n      False\n      2018-06-01T05:51:44Z\n      2023-01-18T12:49:05Z\n      ...\n      [automated-machine-learning, automl, bayesian-...\n      public\n      1736\n      290\n      12415\n      master\n      {'admin': False, 'maintain': False, 'push': Fa...\n      1.0\n      microsoft\n      nni\n    \n    \n      4\n      192640529\n      MDEwOlJlcG9zaXRvcnkxOTI2NDA1Mjk=\n      label-studio\n      False\n      {'login': 'heartexlabs', 'id': 48309720, 'node...\n      https://github.com/heartexlabs/label-studio\n      Label Studio is a multi-type data labeling and...\n      False\n      2019-06-19T02:00:44Z\n      2023-01-18T11:31:18Z\n      ...\n      [annotation, annotation-tool, annotations, bou...\n      public\n      1362\n      448\n      11747\n      develop\n      {'admin': False, 'maintain': False, 'push': Fa...\n      1.0\n      heartexlabs\n      label-studio\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1005\n      534175875\n      R_kgDOH9bggw\n      lightning-hydra-template\n      False\n      {'login': 'aiplaybookin', 'id': 83638338, 'nod...\n      https://github.com/aiplaybookin/lightning-hydr...\n      ML/DL Template with Hydra - DVC, Hyperparamete...\n      False\n      2022-09-08T11:15:13Z\n      2022-12-03T14:02:39Z\n      ...\n      [deep-learning, machine-learning, mlops, mlops...\n      public\n      0\n      5\n      0\n      main\n      {'admin': False, 'maintain': False, 'push': Fa...\n      1.0\n      aiplaybookin\n      lightning-hydra-template\n    \n    \n      1008\n      336675932\n      MDEwOlJlcG9zaXRvcnkzMzY2NzU5MzI=\n      airjordan\n      False\n      {'login': 'kevinmchan', 'id': 2401363, 'node_i...\n      https://github.com/kevinmchan/airjordan\n      An airflow pipeline for building and scoring N...\n      False\n      2021-02-07T01:40:15Z\n      2021-03-29T03:47:21Z\n      ...\n      [airflow, etl-pipeline, ml, mlops, nba]\n      public\n      0\n      13\n      0\n      main\n      {'admin': False, 'maintain': False, 'push': Fa...\n      1.0\n      kevinmchan\n      airjordan\n    \n    \n      1015\n      477140264\n      R_kgDOHHCVKA\n      DataWorkstation.jl\n      False\n      {'login': 'leferrad', 'id': 10536218, 'node_id...\n      https://github.com/leferrad/DataWorkstation.jl\n      A Julia framework to produce your data project...\n      False\n      2022-04-02T18:34:24Z\n      2022-08-14T20:34:58Z\n      ...\n      [data-science, julia, machine-learning, mlops,...\n      public\n      0\n      1\n      0\n      main\n      {'admin': False, 'maintain': False, 'push': Fa...\n      1.0\n      leferrad\n      DataWorkstation.jl\n    \n    \n      1026\n      389434090\n      MDEwOlJlcG9zaXRvcnkzODk0MzQwOTA=\n      Rossmann-Pharmaceuticals-Sales-prediction\n      False\n      {'login': 'Azariagmt', 'id': 56393921, 'node_i...\n      https://github.com/Azariagmt/Rossmann-Pharmace...\n      Time series sales forecast for Rossmann Pharma...\n      False\n      2021-07-25T20:19:38Z\n      2021-08-04T11:02:24Z\n      ...\n      [data-analysis, mlops, regression, rossmann, t...\n      public\n      0\n      1\n      0\n      main\n      {'admin': False, 'maintain': False, 'push': Fa...\n      1.0\n      Azariagmt\n      Rossmann-Pharmaceuticals-Sales-prediction\n    \n    \n      1038\n      551666424\n      R_kgDOIOHC-A\n      mlops\n      False\n      {'login': 'paulsilcock', 'id': 12411602, 'node...\n      https://github.com/paulsilcock/mlops\n      A scratch space to experiment with MLOps techn...\n      False\n      2022-10-14T21:10:03Z\n      2022-10-29T15:14:53Z\n      ...\n      [argo-workflows, docker, dvc, github-actions, ...\n      public\n      0\n      3\n      0\n      main\n      {'admin': False, 'maintain': False, 'push': Fa...\n      1.0\n      paulsilcock\n      mlops\n    \n  \n\n368 rows × 45 columns\n\n\n\nScope: looking at just tools.\nHow to prove if something is an OSS tool?\n\nhas a license\nhas at least one open issue (probably not a strong criteria)\nis public\n\nthink of removing\n\nlists such as “awesome mlops” esque\nprojects (will probably be at least partially removed with no license?)\n\nWhat other tags to collect?\n\nmlops\nmodel-management\n\nOther metrics to bring in?\n\ncontributors"
  },
  {
    "objectID": "proposal/index.html",
    "href": "proposal/index.html",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "",
    "text": "Machine learning operations, or MLOps, is a set of practices to deploy and maintain machine learning models in production reliably and efficiently. This paper will explore the open source MLOps ecosystem as a whole and highlight tools of interest. MLOps tools will be compared and classified by task and maturity. Finally, there will be an overview of key players in the space. It seeks to highlight the knowledge necessary for practitioners and academics to select an open source machine learning operations tool (or tools) to suit their data science projects and machine learning systems."
  },
  {
    "objectID": "proposal/index.html#the-interdisciplinary-nature-of-proposed-research",
    "href": "proposal/index.html#the-interdisciplinary-nature-of-proposed-research",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "The interdisciplinary nature of proposed research",
    "text": "The interdisciplinary nature of proposed research\nMachine learning operations have a naturally interdisciplinary nature. In fact, many of the reasons effectively implementing MLOps solutions is so difficult is due to the span of knowledge necessary for successful operationalization of models. In researching this topic, it is necessary to have knowledge of data wrangling and statistical modeling to create the model (as taught in CAP 5320 - Data Wrangling and Exploratory Data Analysis and CAP 5765 - Computational Data Analysis), along with knowledge of software engineering best practices to have a stable deployment (CEN 5035 - Advanced Software Engineering) and visualization skills to continually monitor model performance (CAP 5735 - Data Visualization and Reproducible Research)."
  },
  {
    "objectID": "proposal/index.html#expected-outcomes",
    "href": "proposal/index.html#expected-outcomes",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Expected outcomes",
    "text": "Expected outcomes\n\nResearch paper\nInteractive dashboard\n\nThrough the expected outcomes, the implications of machine learning operations in open source software will be clear. The paper will lay out the types of tools available for operationalization of models, clustering of types of tools, and an analysis of the maturity of the ecosystem. The dashboard will give an interactive way to explore the data and maturity of the machine learning operations open source ecosystem."
  },
  {
    "objectID": "proposal/index.html#objective-timeline",
    "href": "proposal/index.html#objective-timeline",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Objective timeline",
    "text": "Objective timeline\nAs seen in Figure 2, this project will be broken up into the following parts: literature review, prepare project proposal, gather data, clean and explore data, build maturity models, build unsupervised learning models, build interactive dashboard, and write final report. The literature review and project proposal will be finished by December 2023. Starting in December 2022, data collection of many open source projects will begin. The first task will be to curate a list of projects, sources such as blog posts and GitHub tags will be used. From this list of repositories, further data on these tools can be scraped from the GitHub API. This data will give information such as: description, stars, number of contributors, forks, language, open issues, etc. Once gathered, exploratory data analysis is expected to begin January 2023e. By mid-February, it is expected to be starting on building unsupervised learning models for categorizing tools, maturity models for determining maturity, and a dashboard for an interactive experience to explore the data and models. Starting in March, the final report will be written, to be presented late-April.\n\n\nCode\nimport pandas as pd\nfrom siuba import *\nfrom plotnine import *\n\nobjectives = pd.DataFrame({\n  'task': ['review relevant literature', 'prepare project proposal', 'gather data', 'data cleaning and exploration', 'build maturity models', 'build unsupervised learning models', 'build interactive dashboard', 'author final report'],\n  'start': pd.to_datetime(['2022-08-30', '2022-10-15', '2022-12-01', '2023-01-09', '2023-02-27', '2023-02-27', '2023-03-01', '2023-03-17']),\n  'end': pd.to_datetime(['2022-11-30', '2022-11-30', '2023-01-23', '2023-02-27', '2023-03-27', '2023-03-27', '2023-03-27', '2023-04-17'])\n})\n\n(objectives\n  >> ggplot(aes(x='start', xend='end', y='task', yend='task'))\n    + geom_segment(size=15, color='#a7a9ac')\n    + xlab('date')\n    + ylab('')\n    + ggtitle('Timeline of project completion')\n    + theme_tufte()\n    + theme(axis_text_x=element_text(angle=15))\n  )\n\n\n\n\n\nFigure 2: Project Gantt chart\n\n\n\n\n<ggplot: (402540017)>"
  },
  {
    "objectID": "proposal/index.html#further-work",
    "href": "proposal/index.html#further-work",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Further work",
    "text": "Further work\nThe discoveries from this work could easily be extended further. One possible outcome could be an opinionated workflow for a specific task built with a selection of the tools analyzed; this workflow would contribute to these open source tools through documentation and tutorials, if not also code contributions. It is possible that this software stack could be used to build an accompanying case study, either to be included inside this paper or as part of another work. Another outcome could be a second paper in the style of “Ten simple rules for X”, where the topic would be related to machine learning operations."
  },
  {
    "objectID": "proposal/proposal.html#section",
    "href": "proposal/proposal.html#section",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "",
    "text": "MLOps, is a set of practices to deploy and maintain machine learning models in production reliably and efficiently.\n\nand that is partially because it is so new! if we use google trends, we can see that in late 2021 ish, google searches for mlops started to increase–\nhowever, academic papers on mlops are still scarce, if you were to search google scholar for mlops, you would only get about 1,500 results, where machine learning operations has well over 3.5 million.\nmachine learning operations is important– they are a set of practices to …\nbringing machine learning models outside of local development into a production environment is becoming even more important as machine learning models are being integrated in many services that affect our everyday lives"
  },
  {
    "objectID": "proposal/proposal.html#expected-outcomes",
    "href": "proposal/proposal.html#expected-outcomes",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Expected outcomes",
    "text": "Expected outcomes\nThis project will explore the open source machine learning operations ecosystem as a whole and highlight tools of interest. MLOps tools will be compared and classified by task and maturity.\n\n\nResearch paper\nInteractive dashboard\n\n\ntasks will be classified by unsupervised learning methods\n\norchestration\nml platform\nmodel versioning\nmodel services\nmodel performance monitoring\n\nmaturity will be proxied by github interactions"
  },
  {
    "objectID": "proposal/proposal.html#steps",
    "href": "proposal/proposal.html#steps",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Steps",
    "text": "Steps\n\n\nGather data\nClean and explore data\nBuild maturity models\nBuild unsupervised learning models\nBuild interactive dashboard"
  }
]