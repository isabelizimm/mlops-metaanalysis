[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Machine learning operations, or MLOps, is a new field. It is clear that open-source tools exist and span a variety of tasks to help practitioners bring models into production. Knowing the maturity of this ecosystem can help guide toolmakers to prioritize support and inform academic content on what tasks are being performed in real-world machine learning systems. The data science workflow for static data is well established, but continuously updating data, making predictions on streaming data, and effectively utilizing feedback loops have created new challenges. In all, this paper offers a novel approach by exploring MLOps using a data-driven approach, collecting high-level information on many tools to better understand the size of the ecosystem, the types of tools, and how they are being used. The aim is to better understand the open-source MLOps ecosystem as a whole and see how the code shared reflects the identified sectors in research."
  },
  {
    "objectID": "index.html#slides",
    "href": "index.html#slides",
    "title": "",
    "section": "Slides",
    "text": "Slides\n\n\n\n\n\n ‚ÄÇView slides in new window  ‚ÄÇDownload PDF"
  },
  {
    "objectID": "slides/index.html#section",
    "href": "slides/index.html#section",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "",
    "text": "data science workflow is well established"
  },
  {
    "objectID": "slides/index.html#section-1",
    "href": "slides/index.html#section-1",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "",
    "text": "as projects get bigger you might need to track data"
  },
  {
    "objectID": "slides/index.html#section-2",
    "href": "slides/index.html#section-2",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "",
    "text": "creating many models might have to track each one for forwards/backwards\nhigh quantity, might want experiment tracking with relevant metadata as well"
  },
  {
    "objectID": "slides/index.html#section-3",
    "href": "slides/index.html#section-3",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "",
    "text": "the ROI of models are 0 unless they are used‚Ä¶being used means many different things BUT deployment is when model is off your comptuer"
  },
  {
    "objectID": "slides/index.html#section-4",
    "href": "slides/index.html#section-4",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "",
    "text": "high velocity, high quantity, you don‚Äôt want to manage and run all of this manually.\nif you think of data science as a hierarchy of needs, MLOps is pretty high on the list‚Äì that is, you need a strong foundation of data, analysis and model building before it makes sense to use MLOps.\n\ndata centric\nmodel centric\npipeline centric\n\n40% of data scientists from a study say they work with infrastructure, so pieces are beginning to be used more widely as the maturation of data science progresses\nthere‚Äôs not always a 1:1 ratio of task:tool"
  },
  {
    "objectID": "slides/index.html#github-data",
    "href": "slides/index.html#github-data",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Github data",
    "text": "Github data\n\n\n\n\n\n\n\n\nused data from github, platform that hosts code most open source world is here\n\ndescription\nlabels\nstars (think like facebook or instagram likes, good to see engagement, not good to see quality or usefulness)\nissues\npull requests"
  },
  {
    "objectID": "slides/index.html#github-data-1",
    "href": "slides/index.html#github-data-1",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Github data",
    "text": "Github data\n\ngathered repositories with mlops, model-management labels\nprogrammatic access through REST API to get information\n\ngh api \\ \n  -H \"Accept: application/vnd.github+json\" \\\n  \"/search/repositories?q=topic:mlops&per_page=100&page=1\"\n\ngive back large chunks of json that included metrics like above: number of issues as well as issues content, etx"
  },
  {
    "objectID": "slides/index.html#limitations-of-data",
    "href": "slides/index.html#limitations-of-data",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Limitations of data",
    "text": "Limitations of data\n\nall open source users\nbased off of self-labeled topics\n\n\nThis data is subject to selection bias as the people who are contributing to the open source space likely do not encapsulate a large section of MLOps users, who are likely embedded in organizations working on proprietary models.\nIt is possible that certain languages are over- or under-represented depending on community adoption of topic utilization."
  },
  {
    "objectID": "slides/index.html#exploratory-data-analysis",
    "href": "slides/index.html#exploratory-data-analysis",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\n\nlargeeeely a python space"
  },
  {
    "objectID": "slides/index.html#exploratory-data-analysis-1",
    "href": "slides/index.html#exploratory-data-analysis-1",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\n\nsome of the first paper you can find on google scholar is 2019"
  },
  {
    "objectID": "slides/index.html#modeling",
    "href": "slides/index.html#modeling",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Modeling",
    "text": "Modeling\n\n\nused nltk and scikit-learn\nTFIDF Term frequency‚Äìinverse document frequency - term weighting scheme where the amount of times a word appears in a single document is measured against how often the word is used in a collection of documents\nLatent Dirichlet Allocation (LDA) - simultaneously finds the mixture of words associated with each topic, while also determining the mixture of topics that describes each document\n\nalso tried this with issue and pull requests, but i quickly realized that pretty much all the topics were about maintenance\n\nprincipal component analysis (PCA) - dimensionality reduction technique that maximizes variance while generating new, uncorrelated variables through solving eigenvalue/eigenvector computations - transform the columns into two principal components in order to be visualized in a two-dimensional space.\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) - data seemed to have clusters of dense points after PCA - clusters data points that are high in density; it is especially useful when clusters are high-density areas separated by low-density areas"
  },
  {
    "objectID": "slides/index.html#modeling-1",
    "href": "slides/index.html#modeling-1",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Modeling",
    "text": "Modeling\n\n\n\nunsupervised learning so no known labels to have things like accuracy, RMSE\ngenerated a silhoutte score, which penalizes the overlap of each cluster‚Äôs space\n\ncluster -1 black - outliers - highest amount of average stars - highest amount of average issues - LDA mode: curated lists on production tools\nmost hype is around lists, educational content\ncluster 0 red - average last commit of 256 days - 6 open issues - avg age 1.5 years - commits in the 8 is months - AWS, education small projects actively maintained, demos\ncluster 1 orange - more stars than 0 - more issues than 0 - older than 0 - AWS, deployment\ncluster 2 yellow - less stars than 2 - deep learning\ncluster 3 green - no issues - likely mostly pet projects - AWS, deployment close distance between average age and average time since last commit, this seemed to be largely abandoned repos\ncluster 4 cyan - older repos - lowest stars - versioning, education"
  },
  {
    "objectID": "slides/index.html#how-is-this-ecosystem-growing",
    "href": "slides/index.html#how-is-this-ecosystem-growing",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "How is this ecosystem growing?",
    "text": "How is this ecosystem growing?\n\n\n\nRecency\n\n\n\n\n\n\n‚Äúnot dissimilar to looking at the date that a news article was published before deciding to cite information‚Äù\nbecause this is a young ecosystem, it‚Äôs not surprising that many contributions have been in the last year, but this does give the impression much of this code is pretty fresh"
  },
  {
    "objectID": "slides/index.html#how-is-this-ecosystem-growing-1",
    "href": "slides/index.html#how-is-this-ecosystem-growing-1",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "How is this ecosystem growing?",
    "text": "How is this ecosystem growing?\n\n\n\nRecency\nSize\n\n\n\n\n\n\nPeople can participate in open-source software at multiple levels.\n\nusers, who are utilizing the code posted for tasks\nissue openers\ncode contributors"
  },
  {
    "objectID": "slides/index.html#how-is-this-ecosystem-growing-2",
    "href": "slides/index.html#how-is-this-ecosystem-growing-2",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "How is this ecosystem growing?",
    "text": "How is this ecosystem growing?\n\n\n\nRecency\nSize\nContribution rate\n\n\n\n\n\n\ngross product pull requests (GPPR) GPPR is defined as the number of pull requests merged in a month\nOne repository with 1 dedicated maintainer and 9 contributors who made single pull requests looks very different than another repository with 10 dedicated maintainers, or yet another with 10 few-time contributors.\nsteady and continued growth"
  },
  {
    "objectID": "slides/index.html#what-kind-of-content-is-being-posted",
    "href": "slides/index.html#what-kind-of-content-is-being-posted",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "What kind of content is being posted?",
    "text": "What kind of content is being posted?\n\n\ni generalized\nwhile you do see many repeated topics, this is intentional and for presentation purposes. when you dive into each topic, i found the granularity of each to be individually important: for example, one education topic might be about curated lists of tools, where another education topic is about code demos, etc\nmuch of this exploration leads to the fact that MLOps is not an established space. people are just starting to learn, experiment, try out examples, teach others, etc. maybe, because this a space is a big conglomeration of established principles like devops and machine learning and reproducible work, there is no traditional pathway where you can learn definitions. MLOps practices are generally only implemented at scale, so mocking out large systems makes this a difficult space."
  },
  {
    "objectID": "slides/index.html#what-does-adoption-of-mlops-look-like",
    "href": "slides/index.html#what-does-adoption-of-mlops-look-like",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "What does adoption of MLOps look like?",
    "text": "What does adoption of MLOps look like?\n\n\ncloud prevalence\nDataOps vs MLOps\n\n\n\nthere‚Äôs a great quote from Chip Huyen‚Äôs book Designing Machine Learning Systems: Data versioning is like flossing your teeth. Everyone knows you‚Äôre supposed to do it‚Ä¶"
  },
  {
    "objectID": "project/04_modeling.html",
    "href": "project/04_modeling.html",
    "title": "LDA",
    "section": "",
    "text": "import pandas as pd\nfrom siuba import *\nfrom siuba.siu import call\nfrom plotnine import *\nimport json\nimport matplotlib.pyplot as pp\nimport tidytext\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport string\nimport random\nimport nltk\nfrom nltk import FreqDist\nfrom nltk import ngrams\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nnp.random.seed(500)\n\n\nimport pins\n\nboard = pins.board_folder(\".\")\n\nnumeric_df = board.pin_read(\"train_data\")\n\n\nnumeric_df.drop(columns=[\"allow_forking\"], inplace=True)\nnonone = (numeric_df \n    &gt;&gt; select(_.description)\n    &gt;&gt; filter(-_.description.isin([None])))\n\n\nimport re\ndef remove_emojis(data):\n    emoj = re.compile('[^a-zA-Z]', re.UNICODE)\n    return re.sub(emoj, ' ', data)\n\n# pattern = re.compile('[^A-z0-9 ]+')\n# def clean_text(string):\n#     return pattern.search('', string)\n    \nfrom sklearn.feature_extraction import text\nstop_words = text.ENGLISH_STOP_WORDS.union(frozenset({'b', 's'}))\n\n\ntokenizer = RegexpTokenizer(r'\\w+')\nsents  = nonone.description.to_list()\nregex_sents = []\nfor sent in sents:\n    regex_sents.append(remove_emojis(sent))\n#    clean_text(sent)\n\n# Vectorize document using TF-IDF\ntfidf = TfidfVectorizer(lowercase=True,\n                        stop_words=stop_words,\n                        ngram_range = (1,1),\n                        tokenizer = tokenizer.tokenize)\n\n# Fit and Transform the documents\ntrain_data = tfidf.fit_transform(regex_sents) \n\n\n# visualize topics\nimport matplotlib.pyplot as plt\n\ndef plot_top_words(model, feature_names, n_top_words, title):\n    fig, axes = plt.subplots(5, 4, figsize=(30, 30), sharex=True)\n    axes = axes.flatten()\n    for topic_idx, topic in enumerate(model.components_):\n        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n        top_features = [feature_names[i] for i in top_features_ind]\n        weights = topic[top_features_ind]\n\n        ax = axes[topic_idx]\n        ax.barh(top_features, weights, height=0.7)\n        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 28})\n        ax.invert_yaxis()\n        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n        for i in \"top right left\".split():\n            ax.spines[i].set_visible(False)\n        fig.suptitle(title, fontsize=40)\n\n    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n    plt.show()\n\n\n# https://stackoverflow.com/questions/47370795/pca-on-sklearn-how-to-interpret-pca-components\n# good PCA refresher https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e\n\n\ndef pca_plot(transformed_data, components):\n\n    x = transformed_data[:, 0]\n    y = transformed_data[:, 1]\n    n = components.shape[0]\n\n    plt.scatter(x, y)\n    plt.xlabel(\"PC{}\".format(1))\n    plt.ylabel(\"PC{}\".format(2))\n    plt.grid()\n    \n    for i in range(n):\n        plt.arrow(0, 0, components[i, 0], components[i, 1], color=\"black\", alpha=0.5)\n        plt.text(\n            components[i, 0] * 1.3,\n            components[i, 1] * 1.3,\n            \"Var \" + str(i + 1),\n            color=\"black\",\n            ha=\"center\",\n            va=\"center\",\n        )\n\ndef most_important_feature(components, data):\n    most_important = [np.abs(components[i]).argmax() for i in range(components.shape[0])]\n\n    initial_feature_names = data.columns\n\n    most_important_names = [initial_feature_names[most_important[i]] for i in range(components.shape[0])]\n\n    dic = {'PC{}'.format(i): most_important_names[i] for i in range(components.shape[0])}\n\n\n    return pd.DataFrame(dic.items())\n\n\n# model_05=LatentDirichletAllocation(n_components=5)\n# get_topics(model_05, tfidf, train_data)\n\n\n# model_10=LatentDirichletAllocation(n_components=10)\n# get_topics(model_10, tfidf, train_data)\n\n\n# model_12=LatentDirichletAllocation(n_components=12)\n# get_topics(model_12, tfidf, train_data)\n\n\ndef get_topics(lda_model, tfidf_model, data):\n    terms = tfidf_model.get_feature_names_out()\n    # Fit and Transform SVD model on data\n    lda_matrix = lda_model.fit_transform(data)\n    lda_components = lda_model.components_\n\n    for index, component in enumerate(lda_components):\n        zipped = zip(terms, component)\n        top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:7]\n        top_terms_list=list(dict(top_terms_key).keys())\n        print(\"Topic \"+str(index)+\": \",top_terms_list)\n    \n    return lda_matrix, lda_components\n\n\nmodel_20=LatentDirichletAllocation(n_components=20)\nmatrix, components = get_topics(model_20, tfidf, train_data)\n\nTopic 0:  ['implementation', 'analysis', 'python', 'ml', 'platform', 'mlops', 'model']\nTopic 1:  ['end', 'using', 'mlflow', 'mlops', 'aws', 'model', 'example']\nTopic 2:  ['data', 'management', 'model', 'platform', 'industry', 'torchserve', 'prediction']\nTopic 3:  ['tracking', 'experiment', 'model', 'registry', 'simple', 'data', 'rasa']\nTopic 4:  ['machine', 'learning', 'mlops', 'operations', 'python', 'library', 'allows']\nTopic 5:  ['mlops', 'zoomcamp', 'course', 'project', 'learning', 'datatalksclub', 'development']\nTopic 6:  ['framework', 'learning', 'machine', 'model', 'api', 'tools', 'ai']\nTopic 7:  ['platform', 'amazon', 'solution', 'sagemaker', 'ml', 'learning', 'machine']\nTopic 8:  ['end', 'ml', 'data', 'learning', 'machine', 'template', 'science']\nTopic 9:  ['tutorial', 'dvc', 'practices', 'github', 'complete', 'ml', 'deep']\nTopic 10:  ['classification', 'ml', 'project', 'experimentation', 'feature', 'solution', 'architecture']\nTopic 11:  ['model', 'mlops', 'metrics', 'parameters', 'registry', 'visualize', 'azure']\nTopic 12:  ['data', 'python', 'api', 'ml', 'roadmap', 'engineering', 'make']\nTopic 13:  ['azure', 'mlops', 'cloud', 'google', 'learning', 'vertex', 'workshop']\nTopic 14:  ['monitor', 'management', 'model', 'training', 'fast', 'models', 'easy']\nTopic 15:  ['tool', 'data', 'science', 'mlops', 'machine', 'learning', 'command']\nTopic 16:  ['kedro', 'end', 'plugin', 'pipelines', 'running', 'ml', 'deep']\nTopic 17:  ['curated', 'awesome', 'list', 'ml', 'source', 'open', 'tools']\nTopic 18:  ['machine', 'learning', 'specialization', 'kubeflow', 'engineering', 'projects', 'cloud']\nTopic 19:  ['ai', 'aws', 'learning', 'notebook', 'ml', 'cd', 'code']\n\n\n\n# add topic to dataframe\ntopic = []\nfor n in range(matrix.shape[0]):\n    topic.append(matrix[n].argmax())\nnumeric_df[\"lda_topic\"] = topic\n\n\ntfidf.get_feature_names_out()\n\narray(['aalto', 'abalone', 'ability', ..., 'zo', 'zokyo', 'zoomcamp'],\n      dtype=object)\n\n\n\n## rerun with grid of different numbers of topics + document\nplot_top_words(model_20, tfidf.get_feature_names_out(), 20, \"Topics from repository descriptions\")\n\n\n\n\n\nnumeric_df.drop(columns=['description'], inplace=True)\n\n\nScaled PCA\n\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder().fit_transform(numeric_df['lda_topic'])\nscaled_numeric_df = StandardScaler().fit_transform(numeric_df)\npca_numeric_scaled = PCA(n_components=2).fit(scaled_numeric_df)\npca_numeric_scaled_output = pca_numeric_scaled.transform(scaled_numeric_df)\n\n\nfrom sklearn.pipeline import Pipeline\n# from sklearn.cluster import DBSCAN\n# pipe = Pipeline(\n#     [(\"le\", preprocessing.LabelEncoder()), \n#     (\"scaler\", StandardScaler()), \n#     (\"pca\", PCA()),\n#     (\"dbscan\", DBSCAN())]\n# )\n\n\n# pipe.fit(numeric_df)\n\n\nplt.title(\"PCA of MLOps GitHub dataset, scaled numeric columns\")\n\npca_plot(pca_numeric_scaled_output, pca_numeric_scaled.components_) \nplt.show()\n\n\n\n\n\n\nDBSCAN\n\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics\n\ndb = DBSCAN(eps = 0.5, min_samples=5).fit(pca_numeric_scaled_output)\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint(\"Estimated number of clusters: %d\" % n_clusters_)\nprint(\"Estimated number of noise points: %d\" % n_noise_)\n\nEstimated number of clusters: 5\nEstimated number of noise points: 35\n\n\n\nunique_labels = set(labels)\ncore_samples_mask = np.zeros_like(labels, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\ncolors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = labels == k\n\n    xy = pca_numeric_scaled_output[class_member_mask & core_samples_mask]\n    plt.plot(\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=14,\n    )\n\n    xy = pca_numeric_scaled_output[class_member_mask & ~core_samples_mask]\n    plt.plot(\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\n\nplt.title(f\"DBSCAN estimated number of clusters: {n_clusters_}\")\nplt.show()\n\n\n\n\n\ncore_samples_mask\n\narray([False, False, False, False, False,  True, False, False, False,\n        True, False, False, False, False,  True, False, False, False,\n        True,  True, False,  True,  True, False,  True, False, False,\n        True,  True,  True, False,  True,  True,  True,  True,  True,\n        True, False, False,  True, False,  True,  True,  True,  True,\n        True,  True, False,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True, False,  True,  True,  True,  True,\n        True,  True,  True, False,  True,  True, False,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n       False,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True, False,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True, False,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True, False,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True, False,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True, False,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True, False, False,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True, False,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n       False,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True, False,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True, False,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n       False,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True, False,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True, False,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True, False, False,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True, False,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True, False,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True])\n\n\n\nnumeric_df['clustering_labels'] = db.fit_predict(pca_numeric_scaled_output)\n\n\nfrom sklearn import metrics\nmetrics.silhouette_score(pca_numeric_scaled_output, numeric_df['clustering_labels'])\n\n0.6171555594386241\n\n\ndoesn‚Äôt have overlapping clusters or mislabeled data points.\n\nnumeric_df.columns\n\nIndex(['stargazers_count', 'has_issues', 'has_projects', 'has_downloads',\n       'has_wiki', 'has_pages', 'has_discussions', 'open_issues_count',\n       'is_template', 'age_days', 'time_since_last_commit_days', 'lda_topic',\n       'clustering_labels'],\n      dtype='object')\n\n\n\n(numeric_df\n    &gt;&gt; group_by(_.clustering_labels)\n    &gt;&gt; summarize(\n        avg_stars = _.stargazers_count.mean(),\n        avg_has_issues = _.has_issues.mean(),\n        avg_has_projects = _.has_projects.mean(),\n        avg_has_downloads = _.has_downloads.mean(),\n        avg_has_wiki = _.has_wiki.mean(),\n        avg_has_pages = _.has_pages.mean(),\n        avg_has_discussions = _.has_discussions.mean(),\n        avg_open_issues_count = _.open_issues_count.mean(),\n        avg_is_template = _.is_template.mean(),\n        avg_age_days = _.age_days.mean(),\n        avg_last_commit = _.time_since_last_commit_days.mean(),\n        most_lda_topic = _.lda_topic.mode()[0] + 1\n    )\n)\n\n\n\n\n\n\n\n\nclustering_labels\navg_stars\navg_has_issues\navg_has_projects\navg_has_downloads\navg_has_wiki\navg_has_pages\navg_has_discussions\navg_open_issues_count\navg_is_template\navg_age_days\navg_last_commit\nmost_lda_topic\n\n\n\n\n0\n-1\n4891.057143\n0.742857\n0.771429\n1.000000\n0.485714\n0.314286\n0.514286\n255.685714\n0.028571\n1215.228571\n32.085714\n18\n\n\n1\n0\n125.140948\n1.000000\n1.000000\n1.000000\n1.000000\n0.109356\n0.094775\n6.257594\n0.031592\n533.911300\n256.284326\n2\n\n\n2\n1\n242.562500\n1.000000\n0.906250\n1.000000\n0.093750\n0.312500\n0.312500\n23.781250\n0.062500\n489.625000\n107.781250\n8\n\n\n3\n2\n85.085106\n1.000000\n0.021277\n0.978723\n0.000000\n0.170213\n0.212766\n8.255319\n0.021277\n586.659574\n117.106383\n17\n\n\n4\n3\n52.166667\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n239.333333\n147.666667\n2\n\n\n5\n4\n47.833333\n0.000000\n0.000000\n1.000000\n0.000000\n0.166667\n0.000000\n8.666667\n0.000000\n1483.833333\n8.333333\n10\n\n\n\n\n\n\n\n\n(numeric_df\n    &gt;&gt; count(_.lda_topic)\n    &gt;&gt; arrange(-_.n))\n\n\n\n\n\n\n\n\nlda_topic\nn\n\n\n\n\n1\n1\n107\n\n\n5\n5\n69\n\n\n17\n17\n69\n\n\n18\n18\n63\n\n\n8\n8\n53\n\n\n16\n16\n50\n\n\n11\n11\n49\n\n\n19\n19\n49\n\n\n7\n7\n47\n\n\n4\n4\n45\n\n\n6\n6\n41\n\n\n0\n0\n37\n\n\n2\n2\n37\n\n\n9\n9\n37\n\n\n13\n13\n37\n\n\n12\n12\n35\n\n\n10\n10\n33\n\n\n3\n3\n32\n\n\n15\n15\n30\n\n\n14\n14\n29"
  },
  {
    "objectID": "project/02_eda.html",
    "href": "project/02_eda.html",
    "title": "Q1: How much are these projects growing?",
    "section": "",
    "text": "import pandas as pd\nfrom siuba import *\nfrom siuba.siu import call\nfrom plotnine import *\nimport json\nimport matplotlib.pyplot as pp\n# load all the data\ncommits_raw = pd.read_pickle(\"../data/graphql/commits_total.pkl\")\ncommits_df = pd.DataFrame()\nfor repo in commits_raw:\n    commits_df = pd.concat([commits_df, pd.DataFrame(repo)])\nissues_total = pd.read_pickle(\"../data/graphql/issues_total.pkl\")\nissuespr_total = pd.read_pickle(\"../data/graphql/issuespr_total.pkl\")\nissues_df = pd.concat((pd.DataFrame(issues_total), pd.DataFrame(issuespr_total)))\nwith open(\"../data/repos/topic_combined.json\", \"r\") as read_file:\n    raw = json.load(read_file)\nrepo_df = pd.DataFrame(raw)\nstar_pickle = pd.read_pickle(\"../data/graphql/stargazers_total.pkl\")\nstar_df = pd.DataFrame(star_pickle)\nScope: looking at just tools.\nHow to prove if something is an OSS tool?\nthink of removing\nWhat other tags to collect?\nOther metrics to bring in?\ngrowth = (\n    commits_df\n    &gt;&gt; arrange(_.committer_date)\n    &gt;&gt; mutate(\n        n_1_commits=1,\n        cumsum_commits=_.groupby(\"repository_id\")[\"n_1_commits\"].transform(\n            pd.Series.cumsum\n        ),\n    )\n    &gt;&gt; ungroup()\n)\n# convert this to datetime2013-05-15T08:35:45+02:00\n(growth\n    &gt;&gt; filter(_.repository_id == _.repository_id.iloc[0])\n    &gt;&gt; mutate(\n        committer_date_dt = call(pd.to_datetime, _.committer_date)\n    )\n    # &gt;&gt; _.set_index(\"committer_date\")\n    # &gt;&gt; _.resample(rule='M', on='committer_date')['cumsum_commits']\n    # &gt;&gt; ggplot()\n    # + geom_line(aes('committer_date', 'cumsum_commits'))\n)#DateTimeIndex('committer_date_dt')#.resample(rule='M', on='committer_date')\n\n\n\n\n\n\n\n\nsha\nrepository_id\nauthor_email\ncommitter_email\nauthor_name\nauthor_date\ncommitter_name\ncommitter_date\nmessage\nn_1_commits\ncumsum_commits\ncommitter_date_dt\n\n\n\n\n6095\n3fa5994f589a6b73d1b435cada74f84ece7788f0\nMDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\njdowling@kth.se\njdowling@kth.se\nJim Dowling\n2013-05-15T08:35:45+02:00\nJim Dowling\n2013-05-15T08:35:45+02:00\nImported KTHFS Dashboard\n1\n1\n2013-05-15 08:35:45+02:00\n\n\n6094\n41f8ffb66dc1a3a8eb60e4921dbad8cac55be4d1\nMDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\njdowling@kth.se\njdowling@kth.se\nJim Dowling\n2013-05-15T08:42:00+02:00\nJim Dowling\n2013-05-15T08:42:00+02:00\nRemoved script for creating jarmon symlink\n1\n2\n2013-05-15 08:42:00+02:00\n\n\n6093\n51fa793432d0126f7c4d821fc4db685a84bef2bb\nMDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\nafzali@kth.se\nafzali@kth.se\nHamidreza Afzali\n2013-05-21T16:45:21+02:00\nHamidreza Afzali\n2013-05-21T16:45:21+02:00\nUpgraded to a newer version copied from kthfs ...\n1\n3\n2013-05-21 16:45:21+02:00\n\n\n6092\n9813c228a0aa6f73a50b21c66a03c34664df63fb\nMDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\njdowling@sics.se\njdowling@sics.se\nJim Dowling\n2013-05-21T17:04:20+02:00\nJim Dowling\n2013-05-21T17:04:20+02:00\nMerge branch 'hamid_upgrade' of ghetto.sics.se...\n1\n4\n2013-05-21 17:04:20+02:00\n\n\n6091\nad3af55e5e33aea81294697273f6571089b6c4d3\nMDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\na.lorenteleal@gmail.com\na.lorenteleal@gmail.com\nalorlea\n2013-05-21T16:23:04+02:00\nalorlea\n2013-05-21T17:20:52+02:00\nadded the async api support\n1\n5\n2013-05-21 17:20:52+02:00\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4\n50049920c9a6fa915d997be1aae15f942f89d4cc\nMDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\normenisan.adrian@gmail.com\nnoreply@github.com\nAlex Ormenisan\n2023-01-19T16:05:29+02:00\nGitHub\n2023-01-19T16:05:29+02:00\n[HWORKS-372] explicit provenance missing commu...\n1\n6092\n2023-01-19 16:05:29+02:00\n\n\n3\n801f45bb8b62d954167f760aa00d266b1f1b9101\nMDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\n55157590+DhananjayMukhedkar@users.noreply.gith...\nnoreply@github.com\nDhananjay Mukhedkar\n2023-01-19T16:56:27+01:00\nGitHub\n2023-01-19T16:56:27+01:00\n[FSTORE-313] fix null check for feature group...\n1\n6093\n2023-01-19 16:56:27+01:00\n\n\n2\n8f291f3a58e07b60a8991b9ac9d870553f23106b\nMDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\nrobin.eric.andersson@gmail.com\nnoreply@github.com\nRobin Andersson\n2023-01-19T16:57:39+01:00\nGitHub\n2023-01-19T16:57:39+01:00\n[FSTORE-388][APPEND] Move Agent role to correc...\n1\n6094\n2023-01-19 16:57:39+01:00\n\n\n1\n2a55ea7bc77e6cbef538e10e63a3bebdd7c97bd8\nMDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\nantonios@logicalclocks.com\nnoreply@github.com\nAntonis Kouzoupis\n2023-01-23T13:08:59+01:00\nGitHub\n2023-01-23T13:08:59+01:00\n[HWORKS-380] Close PEMParser resource (#1288)\n1\n6095\n2023-01-23 13:08:59+01:00\n\n\n0\nc0e6a69309a4f0a14f66dbb81d0ee54c0c3b7979\nMDEwOlJlcG9zaXRvcnkxNDI0MTAzMzE=\n8422705+moritzmeister@users.noreply.github.com\nnoreply@github.com\nMoritz Meister\n2023-01-24T14:11:10+01:00\nGitHub\n2023-01-24T14:11:10+01:00\n[FSTORE-614] Auto Kafka Topic recreation needs...\n1\n6096\n2023-01-24 14:11:10+01:00\n\n\n\n\n6096 rows √ó 12 columns\nsliders for time, select per repo or per cluster (from labels?)"
  },
  {
    "objectID": "project/02_eda.html#stargazers",
    "href": "project/02_eda.html#stargazers",
    "title": "Q1: How much are these projects growing?",
    "section": "Stargazers",
    "text": "Stargazers\n\nlen(star_df.user_id.unique())\n\n72664\n\n\n\nlen(star_df.repository_id.unique())\n\n5\n\n\n\ncount_authors = (\n    star_df[\"user_id\"]\n    .value_counts()\n    .value_counts()\n    .to_frame()\n    .reset_index()\n    .rename(columns={\"index\": \"n_repos_starred\", \"user_id\": \"user_count\"})\n)\n\n(ggplot(count_authors, aes(\"n_repos_starred\", \"user_count\")) + geom_point())\n\n\n\n\npeople tend to only star one framework is it all the same framework?\ndo the people who star more contribute more?\n\n_1 = (\n    star_df[\"user_id\"]\n    .value_counts()\n    .to_frame()\n    .reset_index()\n    .rename(columns={\"index\": \"user_id\", \"user_id\": \"n_repos_starred\"})\n)\n\n\n# just use for joins \n_2 = star_df &gt;&gt; full_join(_, _1, on=\"user_id\")\n_3 = repo_df[[\"full_name\", \"node_id\"]].rename(\n    columns={\"node_id\": \"repository_id\"}\n)\n\n\n## users + how many repos they star + repo name\nstars_repo_name = _2 &gt;&gt; left_join(_, _3, on=\"repository_id\")\nstars_repo_name\n\n\n\n\n\n\n\n\nrepository_id\nuser_id\nuser_login\nstarred_at\nn_repos_starred\nfull_name\n\n\n\n\n0\nMDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\nMDQ6VXNlcjE0ODY3ODE5\nraghuch\n2018-11-21T11:50:55Z\n1\nGokuMohandas/Made-With-ML\n\n\n1\nMDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\nMDQ6VXNlcjEzODgxMDA=\nvishalbelsare\n2018-11-30T14:34:05Z\n3\nGokuMohandas/Made-With-ML\n\n\n2\nMDEwOlJlcG9zaXRvcnkyNDAzMTUwNDY=\nMDQ6VXNlcjEzODgxMDA=\nvishalbelsare\n2020-06-15T08:32:23Z\n3\njina-ai/jina\n\n\n3\nMDEwOlJlcG9zaXRvcnkxMzU2NzM0NTE=\nMDQ6VXNlcjEzODgxMDA=\nvishalbelsare\n2021-07-20T16:35:12Z\n3\nmicrosoft/nni\n\n\n4\nMDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\nMDQ6VXNlcjM1NTM3MDE0\npaulananth\n2018-12-01T00:48:08Z\n1\nGokuMohandas/Made-With-ML\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n86581\nMDEwOlJlcG9zaXRvcnkxOTI2NDA1Mjk=\nMDQ6VXNlcjg3MTE2NTcw\nhyunsungK\n2023-01-25T11:17:15Z\n1\nheartexlabs/label-studio\n\n\n86582\nMDEwOlJlcG9zaXRvcnkxOTI2NDA1Mjk=\nMDQ6VXNlcjcxNDM1OQ==\nourgit\n2023-01-25T14:34:50Z\n1\nheartexlabs/label-studio\n\n\n86583\nMDEwOlJlcG9zaXRvcnkxOTI2NDA1Mjk=\nMDQ6VXNlcjU4MzU5NjI1\nalcompa\n2023-01-25T16:02:09Z\n1\nheartexlabs/label-studio\n\n\n86584\nMDEwOlJlcG9zaXRvcnkxOTI2NDA1Mjk=\nMDQ6VXNlcjExOTQ4MjUw\njugodu\n2023-01-25T19:34:15Z\n1\nheartexlabs/label-studio\n\n\n86585\nMDEwOlJlcG9zaXRvcnkxOTI2NDA1Mjk=\nMDQ6VXNlcjU0NzgzNjA5\nzzhenyu23\n2023-01-25T21:50:00Z\n1\nheartexlabs/label-studio\n\n\n\n\n86586 rows √ó 6 columns\n\n\n\n\nassert len(stars_repo_name.full_name.unique()) == len(star_df.repository_id.unique())"
  },
  {
    "objectID": "project/02_eda.html#what-are-these-tools-doing",
    "href": "project/02_eda.html#what-are-these-tools-doing",
    "title": "Q1: How much are these projects growing?",
    "section": "What are these tools doing?",
    "text": "What are these tools doing?\nie, monitoring, for certain model, etc\nfrom description? README? probably not commits\nlanguages from gh look at .gitattributes (in python world, dependency signals?) ^^"
  },
  {
    "objectID": "project/02_eda.html#how-active-are-these-people",
    "href": "project/02_eda.html#how-active-are-these-people",
    "title": "Q1: How much are these projects growing?",
    "section": "How active are these people?",
    "text": "How active are these people?\n\nwatchers\nusers\nbug reporters\nPRs\ncore maintainers\n\nTODO:\n\n[] pull more data for stargazers\n[] answer more questions\n[] start thinking about modeling\n[] wireframe shiny UI (likely shiny for python)\n\nthings to include for shiny:"
  },
  {
    "objectID": "project/03_feature_engineering.html",
    "href": "project/03_feature_engineering.html",
    "title": "find relevant columns with correlation matricies",
    "section": "",
    "text": "import pandas as pd\nfrom siuba import *\nfrom siuba.siu import call\nfrom plotnine import *\nimport json\nimport matplotlib.pyplot as pp\nimport tidytext\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nimport datetime\n\nimport numpy as np\n\nnp.random.seed(500)\n# load all the data\ncommits_raw = pd.read_pickle(\"../data/graphql/commits_total.pkl\")\ncommits_df = pd.DataFrame()\nfor repo in commits_raw:\n    commits_df = pd.concat([commits_df, pd.DataFrame(repo)])\nissues_total = pd.read_pickle(\"../data/graphql/issues_total.pkl\")\nissuespr_total = pd.read_pickle(\"../data/graphql/issuespr_total.pkl\")\nissues_df = pd.concat((pd.DataFrame(issues_total), pd.DataFrame(issuespr_total)))\nwith open(\"../data/repos/topic_combined.json\", \"r\") as read_file:\n    raw = json.load(read_file)\nrepo_df = pd.DataFrame(raw)\n(repo_df\n    &gt;&gt; distinct(_.id)\n    &gt;&gt; count()\n)\n\n\n\n\n\n\n\n\nn\n\n\n\n\n0\n1007\n# select relevant columns\nselected_df = (\n    repo_df\n    &gt;&gt; select(\n        ~_.endswith(\"url\"),\n        ~_.homepage,\n        ~_.size,\n        ~_.web_commit_signoff_required,\n        ~_.visibility,\n        ~_.score,\n        ~_.default_branch,\n        ~_.permissions,\n    )\n    &gt;&gt; filter(_.private == False, _.archived == False, _.disabled == False)\n    &gt;&gt; select(~_.archived, ~_.disabled)\n)\nselected_df.columns\n\nIndex(['id', 'node_id', 'name', 'full_name', 'private', 'owner', 'description',\n       'fork', 'created_at', 'updated_at', 'pushed_at', 'stargazers_count',\n       'watchers_count', 'language', 'has_issues', 'has_projects',\n       'has_downloads', 'has_wiki', 'has_pages', 'has_discussions',\n       'forks_count', 'open_issues_count', 'license', 'allow_forking',\n       'is_template', 'topics', 'forks', 'open_issues', 'watchers'],\n      dtype='object')\nwatchers == watchers_count == stargazers_count\nforks == forks_count\nissues == open_issues_count\nforks_count &lt;&gt; stargazers_count == 0.851789 corr\n(\n    selected_df\n    &gt;&gt; select(\n        ~_.watchers,\n        ~_.fork,\n        ~_.watchers_count,\n        _.stargazers_count,\n        _.forks_count,\n        ~_.forks,\n        _.open_issues_count,\n        ~_.open_issues,\n        ~_.forks_count,\n        ~_.private\n    )\n).corr()\n\n/var/folders/5w/dhznpltj14n3nxr4fybjj8_w0000gn/T/ipykernel_10565/208152933.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n\n\n\n\n\n\n\n\n\nid\nstargazers_count\nhas_issues\nhas_projects\nhas_downloads\nhas_wiki\nhas_pages\nhas_discussions\nopen_issues_count\nallow_forking\nis_template\n\n\n\n\nid\n1.000000\n-0.289823\n0.014164\n0.063037\n-0.006833\n0.105566\n-0.048536\n-0.134018\n-0.283314\nNaN\n0.011518\n\n\nstargazers_count\n-0.289823\n1.000000\n0.021781\n-0.002645\n0.005431\n-0.092477\n0.100473\n0.172367\n0.452322\nNaN\n-0.016101\n\n\nhas_issues\n0.014164\n0.021781\n1.000000\n0.419024\n-0.004744\n0.328706\n-0.002435\n0.034458\n0.023713\nNaN\n-0.011656\n\n\nhas_projects\n0.063037\n-0.002645\n0.419024\n1.000000\n-0.008671\n0.721320\n-0.032895\n-0.053396\n-0.012836\nNaN\n0.026851\n\n\nhas_downloads\n-0.006833\n0.005431\n-0.004744\n-0.008671\n1.000000\n0.089662\n0.011830\n0.011349\n0.003604\nNaN\n0.005530\n\n\nhas_wiki\n0.105566\n-0.092477\n0.328706\n0.721320\n0.089662\n1.000000\n-0.115052\n-0.167513\n-0.154708\nNaN\n-0.011668\n\n\nhas_pages\n-0.048536\n0.100473\n-0.002435\n-0.032895\n0.011830\n-0.115052\n1.000000\n0.222181\n0.080330\nNaN\n-0.032382\n\n\nhas_discussions\n-0.134018\n0.172367\n0.034458\n-0.053396\n0.011349\n-0.167513\n0.222181\n1.000000\n0.273247\nNaN\n0.024793\n\n\nopen_issues_count\n-0.283314\n0.452322\n0.023713\n-0.012836\n0.003604\n-0.154708\n0.080330\n0.273247\n1.000000\nNaN\n-0.023169\n\n\nallow_forking\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nis_template\n0.011518\n-0.016101\n-0.011656\n0.026851\n0.005530\n-0.011668\n-0.032382\n0.024793\n-0.023169\nNaN\n1.000000\nrefined_df = (\n    selected_df\n    &gt;&gt; select(\n        ~_.watchers,\n        ~_.fork,\n        ~_.watchers_count,\n        _.stargazers_count,\n        _.forks_count,\n        ~_.forks,\n        _.open_issues_count,\n        ~_.open_issues,\n        ~_.forks_count,\n        ~_.private\n    )\n    &gt;&gt; filter(-_.description.isin([None]))\n).drop_duplicates(subset='node_id')\nlen(refined_df)\n\n949\nhttps://github.com/orgs/community/discussions/24442 The difference is that pushed_at represents the date and time of the last commit, whereas the updated_at represents the date and time of the last change the the repository. A change to the repository might be a commit, but it may also be other things, such as changing the description of the repo, creating wiki pages\nfrom siuba.experimental.datetime import floor_date, ceil_date\nrefined_df['created_at'] = refined_df.created_at.astype(\"datetime64[ns]\")\nrefined_df['pushed_at'] = refined_df.pushed_at.astype(\"datetime64[ns]\")\nrefined_df['language'] = refined_df.language.astype(\"category\")\nrefined_df = (refined_df \n\n    &gt;&gt; mutate(\n        age_days = (datetime.datetime(2023, 1, 21) - _.created_at).dt.components.days,\n        time_since_last_commit_days = (datetime.datetime(2023, 1, 21) - _.pushed_at).dt.components.days\n    )\n    )\nrefined_df['time_since_last_commit_days'].value_counts().plot()\n\n&lt;AxesSubplot: &gt;\n(refined_df\n    &gt;&gt; arrange(-_.age_days)\n    &gt;&gt; select(_.full_name, _.age_days))\n\n\n\n\n\n\n\n\nfull_name\nage_days\n\n\n\n\n61\nmlcommons/ck\n2998\n\n\n65\npolyaxon/datatile\n2492\n\n\n21\nweaviate/weaviate\n2487\n\n\n66\npolyaxon/traceml\n2441\n\n\n103\nVertaAI/modeldb\n2284\n\n\n...\n...\n...\n\n\n196\nterrytangyuan/awesome-kubeflow\n11\n\n\n847\nYahyaGrb/mlflow_rasa_track\n10\n\n\n1007\nprondos/ax-ml-ops\n7\n\n\n822\nIAmRiteshKoushik/ML-for-Dummies\n5\n\n\n685\neli64s/gpt_auto_markdown_docs\n4\n\n\n\n\n949 rows √ó 2 columns\nprint (\"age mean: \", refined_df.age_days.mean())\nprint(\"age median: \", refined_df.age_days.median())\nprint(\"age max: \", refined_df.age_days.max())\n\nprint (\"stars mean: \", refined_df.stargazers_count.mean())\nprint(\"stars median: \", refined_df.stargazers_count.median())\nprint(\"stars max: \", refined_df.stargazers_count.max())\n\nage mean:  564.3013698630137\nage median:  485.0\nage max:  2998\nstars mean:  301.9378292939937\nstars median:  4.0\nstars max:  32107\n(refined_df\n    &gt;&gt; select(_.full_name, _.stargazers_count, _.node_id)\n    &gt;&gt; arrange(-_.stargazers_count)\n    &gt;&gt; head(20)\n\n)\n\n\n\n\n\n\n\n\nfull_name\nstargazers_count\nnode_id\n\n\n\n\n0\nGokuMohandas/Made-With-ML\n32107\nMDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\n\n\n1\njina-ai/jina\n17144\nMDEwOlJlcG9zaXRvcnkyNDAzMTUwNDY=\n\n\n100\nmlflow/mlflow\n13455\nMDEwOlJlcG9zaXRvcnkxMzYyMDI2OTU=\n\n\n2\nEthicalML/awesome-production-machine-learning\n13008\nMDEwOlJlcG9zaXRvcnkxNDQ4NjM1MjU=\n\n\n3\nmicrosoft/nni\n12415\nMDEwOlJlcG9zaXRvcnkxMzU2NzM0NTE=\n\n\n4\nheartexlabs/label-studio\n11747\nMDEwOlJlcG9zaXRvcnkxOTI2NDA1Mjk=\n\n\n5\nvisenger/awesome-mlops\n9382\nMDEwOlJlcG9zaXRvcnkyNDQ2MjAyNjk=\n\n\n6\nkedro-org/kedro\n8013\nMDEwOlJlcG9zaXRvcnkxODIwNjc1MDY=\n\n\n7\ngreat-expectations/great_expectations\n7852\nMDEwOlJlcG9zaXRvcnkxMDMwNzE1MjA=\n\n\n8\naws/amazon-sagemaker-examples\n7773\nMDEwOlJlcG9zaXRvcnkxMDc5Mzc4MTU=\n\n\n9\nchiphuyen/machine-learning-systems-design\n7614\nMDEwOlJlcG9zaXRvcnkyMjM4MjQ1NDY=\n\n\n10\nNetflix/metaflow\n6312\nMDEwOlJlcG9zaXRvcnkyMDkxMjA2Mzc=\n\n\n11\ndagster-io/dagster\n6283\nMDEwOlJlcG9zaXRvcnkxMzE2MTk2NDY=\n\n\n12\nwandb/wandb\n5324\nMDEwOlJlcG9zaXRvcnk4NjAzMTY3NA==\n\n\n13\nactiveloopai/deeplake\n5158\nMDEwOlJlcG9zaXRvcnkyMDE0MDM5MjM=\n\n\n14\nDataTalksClub/mlops-zoomcamp\n4741\nR_kgDOGQOKeg\n\n\n15\nbentoml/BentoML\n4440\nMDEwOlJlcG9zaXRvcnkxNzg5NzY1Mjk=\n\n\n16\nallegroai/clearml\n4001\nMDEwOlJlcG9zaXRvcnkxOTExMjYzODM=\n\n\n17\nfeast-dev/feast\n3894\nMDEwOlJlcG9zaXRvcnkxNjExMzM3NzA=\n\n\n18\nqdrant/qdrant\n3699\nMDEwOlJlcG9zaXRvcnkyNjgxNjM2MDk=\nsum(refined_df.stargazers_count)\n\n286539\n(refined_df\n    &gt;&gt; mutate(\n        year = _.time_since_last_commit_days/365\n    )\n    &gt;&gt; filter(_.year &gt;= 1)\n)\n\n\n\n\n\n\n\n\nid\nnode_id\nname\nfull_name\nowner\ndescription\ncreated_at\nupdated_at\npushed_at\nstargazers_count\n...\nhas_pages\nhas_discussions\nopen_issues_count\nlicense\nallow_forking\nis_template\ntopics\nage_days\ntime_since_last_commit_days\nyear\n\n\n\n\n74\n375484255\nMDEwOlJlcG9zaXRvcnkzNzU0ODQyNTU=\nmlplatform-workshop\naporia-ai/mlplatform-workshop\n{'login': 'aporia-ai', 'id': 56439290, 'node_i...\nüç´ Example code for a basic ML Platform based o...\n2021-06-09 20:42:22\n2023-01-11T08:46:26Z\n2021-11-03 10:27:17\n373\n...\nFalse\nFalse\n3\n{'key': 'mit', 'name': 'MIT License', 'spdx_id...\nTrue\nFalse\n[devops, machine-learning, mlops]\n590\n443\n1.213699\n\n\n106\n280770443\nMDEwOlJlcG9zaXRvcnkyODA3NzA0NDM=\nklever\nkleveross/klever\n{'login': 'kleveross', 'id': 68282154, 'node_i...\nCloud Native ML/DL Platform\n2020-07-19 01:34:09\n2022-11-20T12:05:40Z\n2020-09-09 07:04:06\n126\n...\nFalse\nFalse\n3\n{'key': 'apache-2.0', 'name': 'Apache License ...\nTrue\nFalse\n[ai-infra, cloud-native, deep-learning, kubefl...\n915\n863\n2.364384\n\n\n114\n114361956\nMDEwOlJlcG9zaXRvcnkxMTQzNjE5NTY=\nlab\nberingresearch/lab\n{'login': 'beringresearch', 'id': 14840322, 'n...\nA lightweight command line interface for the m...\n2017-12-15 11:09:17\n2022-06-10T18:08:23Z\n2021-01-29 06:25:55\n18\n...\nFalse\nFalse\n0\n{'key': 'apache-2.0', 'name': 'Apache License ...\nTrue\nFalse\n[ai, machine-learning, ml, model-management, m...\n1862\n721\n1.975342\n\n\n123\n269145497\nMDEwOlJlcG9zaXRvcnkyNjkxNDU0OTc=\nMMP-Frontend\nIPVS-AS/MMP-Frontend\n{'login': 'IPVS-AS', 'id': 35486093, 'node_id'...\nA Model Management Platform (MMP) for Industry...\n2020-06-03 16:58:48\n2022-11-10T08:17:16Z\n2020-10-28 18:37:24\n5\n...\nFalse\nFalse\n0\n{'key': 'other', 'name': 'Other', 'spdx_id': '...\nTrue\nFalse\n[industry-40, machine-learning, model-manageme...\n961\n814\n2.230137\n\n\n126\n269140157\nMDEwOlJlcG9zaXRvcnkyNjkxNDAxNTc=\nMMP-Backend\nIPVS-AS/MMP-Backend\n{'login': 'IPVS-AS', 'id': 35486093, 'node_id'...\nA Model Management Platform (MMP) for Industry...\n2020-06-03 16:33:23\n2021-04-21T22:50:40Z\n2020-06-04 08:18:50\n3\n...\nFalse\nFalse\n0\n{'key': 'apache-2.0', 'name': 'Apache License ...\nTrue\nFalse\n[industry-40, machine-learning, model-manageme...\n961\n960\n2.630137\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1012\n432638929\nR_kgDOGcmL0Q\ncoursera-Machine-Learning-Engineering-for-Prod...\nxia0nan/coursera-Machine-Learning-Engineering-...\n{'login': 'xia0nan', 'id': 2216266, 'node_id':...\nRepo for Machine Learning Engineering for Prod...\n2021-11-28 06:41:20\n2021-11-29T01:19:27Z\n2021-11-29 01:19:22\n0\n...\nFalse\nFalse\n0\n{'key': 'mit', 'name': 'MIT License', 'spdx_id...\nTrue\nFalse\n[machine-learning, mlops]\n418\n417\n1.142466\n\n\n1019\n270520128\nMDEwOlJlcG9zaXRvcnkyNzA1MjAxMjg=\nrisk-models-for-medical-prognosis\nhirenhk15/risk-models-for-medical-prognosis\n{'login': 'hirenhk15', 'id': 19776567, 'node_i...\nAn End-to-end Machine Learning System with Use...\n2020-06-08 03:54:09\n2022-03-14T09:10:15Z\n2021-03-12 14:13:20\n0\n...\nFalse\nFalse\n0\n{'key': 'mit', 'name': 'MIT License', 'spdx_id...\nTrue\nFalse\n[analytics, artificial-intelligence, data, dat...\n956\n679\n1.860274\n\n\n1023\n394068528\nMDEwOlJlcG9zaXRvcnkzOTQwNjg1Mjg=\nCICD\nvitorpbarbosa7/CICD\n{'login': 'vitorpbarbosa7', 'id': 57735093, 'n...\nFirst contact with Docker\n2021-08-08 20:58:07\n2022-03-03T11:36:03Z\n2021-08-09 00:47:25\n0\n...\nFalse\nFalse\n0\nNone\nTrue\nFalse\n[container, docker, machine-learning, mlops]\n530\n529\n1.449315\n\n\n1025\n286197790\nMDEwOlJlcG9zaXRvcnkyODYxOTc3OTA=\ntrains-get-stats\nshomratalon/trains-get-stats\n{'login': 'shomratalon', 'id': 14982269, 'node...\nGitHub Action For Retrieving You Experiment Wi...\n2020-08-09 08:31:45\n2020-09-14T15:54:38Z\n2020-09-14 15:54:36\n0\n...\nFalse\nFalse\n0\n{'key': 'apache-2.0', 'name': 'Apache License ...\nTrue\nFalse\n[allegro-trains, deep-learning, machine-learni...\n894\n858\n2.350685\n\n\n1026\n389434090\nMDEwOlJlcG9zaXRvcnkzODk0MzQwOTA=\nRossmann-Pharmaceuticals-Sales-prediction\nAzariagmt/Rossmann-Pharmaceuticals-Sales-predi...\n{'login': 'Azariagmt', 'id': 56393921, 'node_i...\nTime series sales forecast for Rossmann Pharma...\n2021-07-25 20:19:38\n2021-08-04T11:02:24Z\n2021-08-04 09:57:55\n0\n...\nFalse\nFalse\n1\nNone\nTrue\nFalse\n[data-analysis, mlops, regression, rossmann, t...\n544\n534\n1.463014\n\n\n\n\n247 rows √ó 25 columns\n(refined_df\n    &gt;&gt; mutate(\n        year = _.time_since_last_commit_days/365\n    )\n    &gt;&gt; ggplot(aes(x = 'year'))\n       + geom_density(fill = 'grey')\n       + theme_matplotlib()\n       + scale_alpha_continuous(breaks = '0.5')\n       + theme(\n            # axis_title_y=element_blank(),\n            # axis_text_y=element_blank(),\n            # axis_ticks_minor_y=element_blank()\n\n       )\n       + labs(\n        title = \"Time since last contribution\",\n        x = \"Years\",\n        y = \"Density\"\n       )\n)\n\n/Users/isabelzimmerman/.pyenv/versions/3.9.11/envs/pydemo/lib/python3.9/site-packages/plotnine/guides/guides.py:187: PlotnineWarning: Cannot generate legend for the 'alpha' aesthetic. Make sure you have mapped a variable to it\nrefined_df.time_since_last_commit_days.median()\n\n130.0\nrefined_df.time_since_last_commit_days.quantile([.25, .5, 0.75])\n\n0.25     19.0\n0.50    130.0\n0.75    377.0\nName: time_since_last_commit_days, dtype: float64\nrefined_df['age_days'].value_counts().plot()\n\n&lt;AxesSubplot: &gt;\nlang = pd.DataFrame(\n    refined_df.language.value_counts()\n    .reset_index()\n    .rename(columns={\"index\": \"language\", \"language\": \"n\"})\n    .sort_values(by=['n'])\n    .astype({'language': 'category'})\n)\n\ntotal_lang = sum(lang.n)\n#board.pin_write(lang_percent, \"lang\", type=\"csv\")\n(lang \n    &gt;&gt; filter(_.n &gt;= 5)\n    &gt;&gt; arrange(-_.n)\n    &gt;&gt; mutate(\n        percent = _.n/total_lang\n    )\n    &gt;&gt; ggplot()\n        + geom_col(aes(x = 'reorder(language, percent)', y = \"percent\"))\n        + labs(\n            y = 'Number of repositories', \n            x = 'Language',\n            title = 'Repositories for each language')\n        + scale_y_continuous()\n        + coord_flip()\n        + theme_minimal()\n        + theme(axis_text_x = element_text(angle = -75))\n\n    )\n\n/Users/isabelzimmerman/.pyenv/versions/3.9.11/envs/pydemo/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n/Users/isabelzimmerman/.pyenv/versions/3.9.11/envs/pydemo/lib/python3.9/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide"
  },
  {
    "objectID": "project/03_feature_engineering.html#not-scaled",
    "href": "project/03_feature_engineering.html#not-scaled",
    "title": "find relevant columns with correlation matricies",
    "section": "not scaled",
    "text": "not scaled\n\npca_numeric = PCA(n_components=2).fit(numeric_df)\npca_numeric_output = pca_numeric.transform(numeric_df)\n\nValueError: could not convert string to float: 'Learn how to responsibly develop, deploy and maintain production machine learning applications.'\n\n\n\nnumeric_df.dtypes\n\nstargazers_count               int64\nhas_issues                      bool\nhas_projects                    bool\nhas_downloads                   bool\nhas_wiki                        bool\nhas_pages                       bool\nhas_discussions                 bool\nopen_issues_count              int64\nallow_forking                   bool\nis_template                     bool\nage_days                       int64\ntime_since_last_commit_days    int64\ndtype: object\n\n\n\npca_numeric.components_\n\narray([[ 9.95783126e-01,  1.90231055e-06,  1.51537777e-06,\n         1.08088645e-07, -1.47462294e-05,  2.12904286e-05,\n         3.38724621e-05,  2.26738089e-02,  0.00000000e+00,\n        -1.64106884e-06,  8.59427426e-02, -2.27092158e-02],\n       [-7.49244672e-02, -1.14259322e-05, -2.71273552e-05,\n         1.00932636e-06, -3.19559086e-05, -1.89731021e-05,\n         3.01801733e-05,  3.11311337e-02, -0.00000000e+00,\n        -1.40761332e-05,  9.44231553e-01,  3.19129985e-01]])\n\n\n\n# https://stackoverflow.com/questions/47370795/pca-on-sklearn-how-to-interpret-pca-components\n# good PCA refresher https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e\n\n\ndef pca_plot(transformed_data, components):\n\n    x = transformed_data[:, 0]\n    y = transformed_data[:, 1]\n    n = components.shape[0]\n\n    plt.scatter(x, y)\n    plt.xlabel(\"PC{}\".format(1))\n    plt.ylabel(\"PC{}\".format(2))\n    plt.grid()\n    \n    for i in range(n):\n        plt.arrow(0, 0, components[i, 0], components[i, 1], color=\"black\", alpha=0.5)\n        plt.text(\n            components[i, 0] * 1.3,\n            components[i, 1] * 1.3,\n            \"Var \" + str(i + 1),\n            color=\"black\",\n            ha=\"center\",\n            va=\"center\",\n        )\n\ndef most_important_feature(components, data):\n    most_important = [np.abs(components[i]).argmax() for i in range(components.shape[0])]\n\n    initial_feature_names = data.columns\n\n    most_important_names = [initial_feature_names[most_important[i]] for i in range(components.shape[0])]\n\n    dic = {'PC{}'.format(i): most_important_names[i] for i in range(components.shape[0])}\n\n\n    return pd.DataFrame(dic.items())\n\n\nplt.title(\"PCA of MLOps GitHub dataset, numeric columns\")\npca_plot(pca_numeric_output, pca_numeric.components_) \nplt.show()\n\n\n\n\n\nmost_important_feature(pca_numeric.components_, numeric_df)\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nPC0\nstargazers_count\n\n\n1\nPC1\nage_days"
  },
  {
    "objectID": "project/03_feature_engineering.html#scaled",
    "href": "project/03_feature_engineering.html#scaled",
    "title": "find relevant columns with correlation matricies",
    "section": "scaled",
    "text": "scaled\nshould i transform this data? feature scaling https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py\n\nscaled_numeric_df = StandardScaler().fit_transform(numeric_df)\npca_numeric_scaled = PCA(n_components=2).fit(scaled_numeric_df)\npca_numeric_scaled_output = pca_numeric_scaled.transform(scaled_numeric_df)\n\n\nnumeric_df.columns\n\nIndex(['stargazers_count', 'has_issues', 'has_projects', 'has_downloads',\n       'has_wiki', 'has_pages', 'has_discussions', 'open_issues_count',\n       'allow_forking', 'is_template', 'age_days',\n       'time_since_last_commit_days'],\n      dtype='object')\n\n\nwhat does this mean?\n\nplt.title(\"PCA of MLOps GitHub dataset, scaled numeric columns\")\n\npca_plot(pca_numeric_scaled_output, pca_numeric_scaled.components_) \nplt.show()\n\n\n\n\n\n# from https://scikit-learn.org/stable/auto_examples/cluster/plot_bisect_kmeans.html#sphx-glr-auto-examples-cluster-plot-bisect-kmeans-py\n \nfrom sklearn.cluster import KMeans, DBSCAN\nn_clusters_list = [3, 4, 5, 8]\n\nclustering_algorithms = {\n    \"K-Means\": KMeans,\n}\n\n# Make subplots for each variant\nfig, axs = plt.subplots(\n    len(clustering_algorithms), len(n_clusters_list), figsize=(12, 5)\n)\n\n\nfor i, (algorithm_name, Algorithm) in enumerate(clustering_algorithms.items()):\n    for j, n_clusters in enumerate(n_clusters_list):\n        algo = Algorithm(n_clusters=n_clusters, random_state=500, n_init=3)\n        algo.fit(pca_numeric_scaled_output)\n        centers = algo.cluster_centers_\n\n        axs[j].scatter(pca_numeric_scaled_output[:, 0], pca_numeric_scaled_output[:, 1], s=10, c=algo.labels_)\n        axs[j].scatter(centers[:, 0], centers[:, 1], c=\"r\", s=20)\n\n        axs[j].set_title(f\"{algorithm_name} : {n_clusters} clusters\")\n\n\n# Hide x labels and tick labels for top plots and y ticks for right plots.\nfor ax in axs.flat:\n    ax.label_outer()\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.show()\n\n\n\n\n\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics\n\ndb = DBSCAN(eps = 0.5, min_samples=5).fit(pca_numeric_scaled_output)\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint(\"Estimated number of clusters: %d\" % n_clusters_)\nprint(\"Estimated number of noise points: %d\" % n_noise_)\n\nEstimated number of clusters: 5\nEstimated number of noise points: 32\n\n\n\nunique_labels = set(labels)\ncore_samples_mask = np.zeros_like(labels, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\ncolors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = labels == k\n\n    xy = pca_numeric_scaled_output[class_member_mask & core_samples_mask]\n    plt.plot(\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=14,\n    )\n\n    xy = pca_numeric_scaled_output[class_member_mask & ~core_samples_mask]\n    plt.plot(\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\n\nplt.title(f\"DBSCAN estimated number of clusters: {n_clusters_}\")\nplt.show()\n\n\n\n\n\nnumeric_df['pca_x'] = pca_numeric_scaled_output[:,0]\nnumeric_df['pca_y'] = pca_numeric_scaled_output[:,1]\n\n\n(numeric_df\n    &gt;&gt; filter(_.pca_x &lt; 0, _.pca_y &lt; 0)\n)\n\n\n\n\n\n\n\n\nstargazers_count\nhas_issues\nhas_projects\nhas_downloads\nhas_wiki\nhas_pages\nhas_discussions\nopen_issues_count\nallow_forking\nis_template\nage_days\ntime_since_last_commit_days\npca_x\npca_y\n\n\n\n\n70\n398\nTrue\nTrue\nTrue\nTrue\nFalse\nFalse\n2\nTrue\nFalse\n238\n2\n-0.531758\n-0.192959\n\n\n71\n396\nTrue\nTrue\nTrue\nTrue\nFalse\nFalse\n1\nTrue\nFalse\n136\n81\n-0.664739\n-0.274867\n\n\n77\n357\nTrue\nTrue\nTrue\nTrue\nFalse\nFalse\n3\nTrue\nFalse\n430\n88\n-0.513025\n-0.041034\n\n\n93\n240\nTrue\nTrue\nTrue\nTrue\nFalse\nFalse\n40\nTrue\nFalse\n261\n2\n-0.398149\n-0.008217\n\n\n117\n11\nTrue\nTrue\nTrue\nTrue\nFalse\nFalse\n1\nTrue\nFalse\n275\n45\n-0.622603\n-0.274088\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1038\n0\nTrue\nTrue\nTrue\nTrue\nFalse\nFalse\n3\nTrue\nFalse\n98\n21\n-0.688753\n-0.407986\n\n\n1039\n0\nTrue\nTrue\nTrue\nTrue\nFalse\nFalse\n0\nTrue\nFalse\n31\n31\n-0.745678\n-0.477094\n\n\n1040\n0\nTrue\nTrue\nTrue\nTrue\nFalse\nFalse\n0\nTrue\nFalse\n305\n10\n-0.579480\n-0.261147\n\n\n1041\n0\nTrue\nTrue\nTrue\nTrue\nFalse\nFalse\n0\nTrue\nFalse\n231\n4\n-0.613402\n-0.320219\n\n\n1042\n0\nTrue\nTrue\nTrue\nTrue\nFalse\nFalse\n0\nTrue\nFalse\n44\n10\n-0.719003\n-0.468135\n\n\n\n\n458 rows √ó 14 columns\n\n\n\n\n(numeric_df\n    &gt;&gt; filter(_.pca_x &gt; 1, _.pca_x &lt; 2, _.pca_y &lt; 0)\n)\n\n\n\n\n\n\n\n\nstargazers_count\nhas_issues\nhas_projects\nhas_downloads\nhas_wiki\nhas_pages\nhas_discussions\nopen_issues_count\nallow_forking\nis_template\nage_days\ntime_since_last_commit_days\npca_x\npca_y\n\n\n\n\n56\n677\nTrue\nFalse\nTrue\nTrue\nFalse\nFalse\n130\nTrue\nFalse\n276\n2\n1.822870\n-0.943839\n\n\n91\n242\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n18\nTrue\nFalse\n548\n83\n1.258243\n-0.812920\n\n\n162\n127\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n13\nTrue\nFalse\n470\n44\n1.213945\n-0.936983\n\n\n196\n79\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n0\nTrue\nFalse\n11\n9\n1.528874\n-0.978110\n\n\n198\n78\nTrue\nTrue\nTrue\nFalse\nFalse\nTrue\n10\nTrue\nFalse\n485\n326\n1.835465\n-0.089116\n\n\n208\n72\nTrue\nFalse\nTrue\nTrue\nFalse\nFalse\n0\nTrue\nFalse\n345\n224\n1.039869\n-1.763245\n\n\n222\n55\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n16\nTrue\nFalse\n121\n2\n1.066207\n-1.219881\n\n\n227\n54\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n5\nTrue\nFalse\n378\n8\n1.154828\n-1.076884\n\n\n291\n25\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n4\nTrue\nFalse\n668\n379\n1.538900\n-0.426156\n\n\n299\n23\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n72\nTrue\nFalse\n1246\n2\n1.880799\n-0.026428\n\n\n314\n20\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n18\nTrue\nFalse\n723\n93\n1.304350\n-0.735272\n\n\n321\n19\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n20\nTrue\nFalse\n290\n5\n1.163181\n-1.073525\n\n\n363\n13\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n11\nTrue\nFalse\n538\n5\n1.845995\n-0.517867\n\n\n369\n12\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n5\nTrue\nFalse\n616\n198\n1.682798\n-0.477111\n\n\n395\n10\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n0\nTrue\nFalse\n768\n32\n1.900114\n-0.395489\n\n\n424\n7\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n0\nTrue\nFalse\n39\n16\n1.524927\n-0.975491\n\n\n509\n4\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n0\nTrue\nFalse\n300\n138\n1.308122\n-2.643825\n\n\n580\n3\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n0\nTrue\nFalse\n240\n220\n1.198851\n-2.686414\n\n\n655\n2\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n0\nTrue\nFalse\n8\n4\n1.518770\n-1.002239\n\n\n675\n1\nTrue\nFalse\nTrue\nTrue\nTrue\nFalse\n0\nTrue\nFalse\n203\n187\n1.572961\n-1.487499\n\n\n767\n1\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n0\nTrue\nFalse\n306\n94\n1.352146\n-2.642730\n\n\n878\n0\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n1\nTrue\nFalse\n450\n177\n1.596108\n-0.635603\n\n\n926\n0\nTrue\nFalse\nTrue\nTrue\nFalse\nFalse\n0\nTrue\nFalse\n632\n186\n1.216645\n-1.558117\n\n\n1043\n0\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\n0\nTrue\nTrue\n984\n3\n1.459667\n-0.790121\n\n\n\n\n\n\n\n\nmost_important_feature(pca_numeric_scaled.components_, numeric_df)\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nPC0\nhas_wiki\n\n\n1\nPC1\nopen_issues_count"
  },
  {
    "objectID": "project/03_feature_engineering.html#what-about-the-self-labeled-topics-at-the-repo-level",
    "href": "project/03_feature_engineering.html#what-about-the-self-labeled-topics-at-the-repo-level",
    "title": "find relevant columns with correlation matricies",
    "section": "what about the self labeled topics at the repo level?",
    "text": "what about the self labeled topics at the repo level?\n\nrefined_df.topics.iloc[0]\n\n['data-engineering',\n 'data-science',\n 'deep-learning',\n 'machine-learning',\n 'mlops',\n 'natural-language-processing',\n 'python',\n 'pytorch']\n\n\n\nfrom collections import Counter\n\nall_topics = (\n    pd.DataFrame.from_dict(Counter(refined_df.topics.sum()), orient=\"index\")\n    .reset_index()\n    .rename(columns={\"index\": \"topics\", 0: \"n\"})\n)\n\n\n(all_topics\n    &gt;&gt; arrange(-_.n)\n)\n\n\n\n\n\n\n\n\ntopics\nn\n\n\n\n\n4\nmlops\n938\n\n\n3\nmachine-learning\n512\n\n\n6\npython\n237\n\n\n1\ndata-science\n195\n\n\n2\ndeep-learning\n169\n\n\n...\n...\n...\n\n\n1739\nfull-stack\n1\n\n\n1740\nplant\n1\n\n\n1741\nplantdiseasedetection\n1\n\n\n1742\nweb\n1\n\n\n1743\nazuremlops\n1\n\n\n\n\n1744 rows √ó 2 columns\n\n\n\n\n(all_topics\n    &gt;&gt; filter(_.n &gt; 1)\n    &gt;&gt; count()\n)\n\n\n\n\n\n\n\n\nn\n\n\n\n\n0\n610\n\n\n\n\n\n\n\n\nall_topics['topics'] = all_topics.topics.str.replace(\"-\", \" \")\nall_topics\n\n\n\n\n\n\n\n\ntopics\nn\n\n\n\n\n0\ndata engineering\n30\n\n\n1\ndata science\n195\n\n\n2\ndeep learning\n169\n\n\n3\nmachine learning\n512\n\n\n4\nmlops\n938\n\n\n...\n...\n...\n\n\n1739\nfull stack\n1\n\n\n1740\nplant\n1\n\n\n1741\nplantdiseasedetection\n1\n\n\n1742\nweb\n1\n\n\n1743\nazuremlops\n1\n\n\n\n\n1744 rows √ó 2 columns"
  },
  {
    "objectID": "project/03_feature_engineering.html#what-about-licenses",
    "href": "project/03_feature_engineering.html#what-about-licenses",
    "title": "find relevant columns with correlation matricies",
    "section": "what about licenses?",
    "text": "what about licenses?\n\nrefined_df['license'] = refined_df.license.apply(pd.Series, dtype = 'object').key\n\n\nrefined_df.license.astype(\"category\")\n\n0              mit\n1       apache-2.0\n2              mit\n3              mit\n4       apache-2.0\n           ...    \n1039           NaN\n1040           NaN\n1041           NaN\n1042           mit\n1043           mit\nName: license, Length: 986, dtype: category\nCategories (13, object): ['agpl-3.0', 'apache-2.0', 'bsd-2-clause', 'bsd-3-clause', ..., 'mit', 'mit-0', 'mpl-2.0', 'other']\n\n\n\nrefined_df\n\n\n\n\n\n\n\n\nid\nnode_id\nname\nfull_name\nowner\ndescription\ncreated_at\nupdated_at\npushed_at\nstargazers_count\n...\nhas_wiki\nhas_pages\nhas_discussions\nopen_issues_count\nlicense\nallow_forking\nis_template\ntopics\nage_days\ntime_since_last_commit_days\n\n\n\n\n0\n156157055\nMDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\nMade-With-ML\nGokuMohandas/Made-With-ML\n{'login': 'GokuMohandas', 'id': 8000987, 'node...\nLearn how to responsibly develop, deploy and m...\n2018-11-05 03:44:27\n2023-01-18T14:24:54Z\n2022-11-08 13:52:44\n32107\n...\nTrue\nFalse\nFalse\n6\nmit\nTrue\nFalse\n[data-engineering, data-science, deep-learning...\n1537\n73\n\n\n1\n240315046\nMDEwOlJlcG9zaXRvcnkyNDAzMTUwNDY=\njina\njina-ai/jina\n{'login': 'jina-ai', 'id': 60539444, 'node_id'...\nüîÆ Build multimodal AI services via cloud nativ...\n2020-02-13 17:04:44\n2023-01-18T14:18:14Z\n2023-01-18 16:21:18\n17144\n...\nFalse\nTrue\nFalse\n33\napache-2.0\nTrue\nFalse\n[aiops, airflow, cloud-native, creative-ai, cr...\n1072\n2\n\n\n2\n144863525\nMDEwOlJlcG9zaXRvcnkxNDQ4NjM1MjU=\nawesome-production-machine-learning\nEthicalML/awesome-production-machine-learning\n{'login': 'EthicalML', 'id': 43532924, 'node_i...\nA curated list of awesome open source librarie...\n2018-08-15 14:28:41\n2023-01-18T17:16:27Z\n2023-01-16 06:42:25\n13008\n...\nTrue\nTrue\nFalse\n26\nmit\nTrue\nFalse\n[awesome, awesome-list, data-mining, deep-lear...\n1619\n4\n\n\n3\n135673451\nMDEwOlJlcG9zaXRvcnkxMzU2NzM0NTE=\nnni\nmicrosoft/nni\n{'login': 'microsoft', 'id': 6154722, 'node_id...\nAn open source AutoML toolkit for automate mac...\n2018-06-01 05:51:44\n2023-01-18T12:49:05Z\n2023-01-18 09:27:55\n12415\n...\nTrue\nFalse\nTrue\n290\nmit\nTrue\nFalse\n[automated-machine-learning, automl, bayesian-...\n1694\n2\n\n\n4\n192640529\nMDEwOlJlcG9zaXRvcnkxOTI2NDA1Mjk=\nlabel-studio\nheartexlabs/label-studio\n{'login': 'heartexlabs', 'id': 48309720, 'node...\nLabel Studio is a multi-type data labeling and...\n2019-06-19 02:00:44\n2023-01-18T11:31:18Z\n2023-01-18 17:09:25\n11747\n...\nTrue\nFalse\nTrue\n448\napache-2.0\nTrue\nFalse\n[annotation, annotation-tool, annotations, bou...\n1311\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1039\n580399205\nR_kgDOIpgwZQ\nstep-functions-sagemaker-ml-pipeline\nhkford/step-functions-sagemaker-ml-pipeline\n{'login': 'hkford', 'id': 82389275, 'node_id':...\nML pipeline with AWS Step Functions and Amazon...\n2022-12-20 13:17:41\n2022-12-20T13:33:21Z\n2022-12-20 13:32:14\n0\n...\nTrue\nFalse\nFalse\n0\nNaN\nTrue\nFalse\n[mlops, sagemaker, step-functions]\n31\n31\n\n\n1040\n472158586\nR_kgDOHCSReg\ngenome\nedeliu2000/genome\n{'login': 'edeliu2000', 'id': 8235794, 'node_i...\nAI AutoML Platform Services for Managing Milli...\n2022-03-21 02:18:15\n2022-12-09T17:27:37Z\n2023-01-10 23:03:03\n0\n...\nTrue\nFalse\nFalse\n0\nNaN\nTrue\nFalse\n[automlops, configurable-ai, controllable-ai, ...\n305\n10\n\n\n1041\n499422453\nR_kgDOHcSU9Q\nml-ops\nkhoaguin/ml-ops\n{'login': 'khoaguin', 'id': 88959106, 'node_id...\nA Learning Journal on Machine Learning in Prod...\n2022-06-03 07:38:42\n2022-06-10T08:40:17Z\n2023-01-16 07:24:02\n0\n...\nTrue\nFalse\nFalse\n0\nNaN\nTrue\nFalse\n[mlops]\n231\n4\n\n\n1042\n575407571\nR_kgDOIkwF0w\nPlant-disease-Detection\nshashank1623/Plant-disease-Detection\n{'login': 'shashank1623', 'id': 86946068, 'nod...\nPlant Disease Detection using convolutional n...\n2022-12-07 12:53:33\n2023-01-10T17:25:44Z\n2023-01-10 17:29:52\n0\n...\nTrue\nFalse\nFalse\n0\nmit\nTrue\nFalse\n[adam-optimizer, cnn, d, deep-learning, develo...\n44\n10\n\n\n1043\n262970855\nMDEwOlJlcG9zaXRvcnkyNjI5NzA4NTU=\naml-templates\nweekoflearning/aml-templates\n{'login': 'weekoflearning', 'id': 63700392, 'n...\ntemplate repository for azure machine learnings\n2020-05-11 07:30:45\n2023-01-17T05:37:09Z\n2023-01-17 05:37:05\n0\n...\nFalse\nFalse\nFalse\n0\nmit\nTrue\nTrue\n[azure-ml, azuremachinelearning, azuremlops, m...\n984\n3\n\n\n\n\n986 rows √ó 24 columns"
  },
  {
    "objectID": "project/03_feature_engineering.html#what-do-we-know-about-the-prs",
    "href": "project/03_feature_engineering.html#what-do-we-know-about-the-prs",
    "title": "find relevant columns with correlation matricies",
    "section": "what do we know about the PRs?",
    "text": "what do we know about the PRs?\n\nprs_df = pd.DataFrame(issuespr_total)\nprs_df.head()\n\n\n\n\n\n\n\n\nid\nmilestone_id\nrepository_id\nuser_id\nbody\nclosed_at\ncreated_at\nlocked\nnumber\nstate\ntitle\nupdated_at\ntype\n\n\n\n\n0\nMDExOlB1bGxSZXF1ZXN0MjI4NDQ0MTI4\nNone\nMDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\nMDQ6VXNlcjgwMDA5ODc=\n\n2018-11-05T18:20:05Z\n2018-11-05T18:14:37Z\nFalse\n1\nMERGED\nadded instructions for running and contributin...\n2018-11-05T18:20:05Z\nPullRequest\n\n\n1\nMDExOlB1bGxSZXF1ZXN0MjI4NDQ2MDM3\nNone\nMDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\nMDQ6VXNlcjgwMDA5ODc=\n\n2018-11-05T18:21:46Z\n2018-11-05T18:21:38Z\nFalse\n2\nMERGED\nfixed upload image link\n2018-11-05T18:21:46Z\nPullRequest\n\n\n2\nMDExOlB1bGxSZXF1ZXN0MjI4NDU3Mjg4\nNone\nMDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\nMDQ6VXNlcjgwMDA5ODc=\n\n2018-11-05T19:02:46Z\n2018-11-05T19:02:25Z\nFalse\n3\nMERGED\nmade readme more readable by adding linebreaks\n2018-11-05T19:02:46Z\nPullRequest\n\n\n3\nMDExOlB1bGxSZXF1ZXN0MjI4NDU3ODMy\nNone\nMDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\nMDQ6VXNlcjgwMDA5ODc=\n\n2018-11-05T19:04:31Z\n2018-11-05T19:04:24Z\nFalse\n4\nMERGED\nfixed contribution instructions\n2018-11-05T19:04:32Z\nPullRequest\n\n\n4\nMDExOlB1bGxSZXF1ZXN0MjI4NDU4NTc0\nNone\nMDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\nMDQ6VXNlcjgwMDA5ODc=\n\n2018-11-05T19:07:24Z\n2018-11-05T19:07:17Z\nFalse\n5\nMERGED\nfixed contributing instructions\n2018-11-05T20:47:30Z\nPullRequest\n\n\n\n\n\n\n\n\n(prs_df\n    &gt;&gt; distinct(_.type))\n\n\n\n\n\n\n\n\ntype\n\n\n\n\n0\nPullRequest\n\n\n\n\n\n\n\n\n(prs_df\n    &gt;&gt; count(_.repository_id)\n    &gt;&gt; arrange(-_.n))\n\n\n\n\n\n\n\n\nrepository_id\nn\n\n\n\n\n19\nMDEwOlJlcG9zaXRvcnkxMzE2MTk2NDY=\n6589\n\n\n36\nMDEwOlJlcG9zaXRvcnkxNzI4MjIxOTU=\n6478\n\n\n21\nMDEwOlJlcG9zaXRvcnkxMzMxMDA4ODA=\n5604\n\n\n90\nMDEwOlJlcG9zaXRvcnkyNTM4NDY4Nzk=\n5572\n\n\n11\nMDEwOlJlcG9zaXRvcnkxMDMwNzE1MjA=\n5320\n\n\n...\n...\n...\n\n\n267\nR_kgDOGxQ4Bg\n1\n\n\n286\nR_kgDOHJgXCw\n1\n\n\n293\nR_kgDOHTLodQ\n1\n\n\n305\nR_kgDOHhZcuw\n1\n\n\n330\nR_kgDOIu169Q\n1\n\n\n\n\n333 rows √ó 2 columns\n\n\n\n\nusers_issues = issues_df &gt;&gt; select(_.user_id) &gt;&gt; distinct()\nusers_prs = prs_df &gt;&gt; select(_.user_id) &gt;&gt; distinct()\n\nprint(f'issues: {len(users_issues)}')\nprint(f'prs: {len(users_prs)}')\n\nissues: 17649\nprs: 7373\n\n\n\n(pd.concat([users_issues, users_prs])\n    &gt;&gt; distinct()\n)\n\n\n\n\n\n\n\n\nuser_id\n\n\n\n\n0\nMDQ6VXNlcjExNTMwMjQ3\n\n\n1\nMDQ6VXNlcjIyOTU5MDM3\n\n\n2\nMDQ6VXNlcjI3MTQzMDU4\n\n\n3\nMDQ6VXNlcjcwMDYxNw==\n\n\n4\nMDQ6VXNlcjExNTU1NzM=\n\n\n...\n...\n\n\n17644\nMDQ6VXNlcjM3NTczNjI5\n\n\n17645\nU_kgDOBt1LjA\n\n\n17646\nU_kgDOBZf1Ng\n\n\n17647\nMDQ6VXNlcjY0NDg3MDM4\n\n\n17648\nMDQ6VXNlcjEwNTM2MjE4\n\n\n\n\n17649 rows √ó 1 columns\n\n\n\n\nassert len(prs_df.columns) == len(issues_df.columns)\n\n\ncombine_issues_prs = pd.concat([issues_df, prs_df])\n\n\n(combine_issues_prs\n    &gt;&gt; distinct(_.repository_id)\n)\n\n\n\n\n\n\n\n\nrepository_id\n\n\n\n\n0\nMDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\n\n\n1\nMDEwOlJlcG9zaXRvcnkyNDAzMTUwNDY=\n\n\n2\nMDEwOlJlcG9zaXRvcnkxNDQ4NjM1MjU=\n\n\n3\nMDEwOlJlcG9zaXRvcnkxMzU2NzM0NTE=\n\n\n4\nMDEwOlJlcG9zaXRvcnkxOTI2NDA1Mjk=\n\n\n...\n...\n\n\n358\nR_kgDOHj3zDw\n\n\n359\nMDEwOlJlcG9zaXRvcnk0MDIzMDY2NzE=\n\n\n360\nR_kgDOHozdZg\n\n\n361\nR_kgDOH9bggw\n\n\n362\nR_kgDOHHCVKA\n\n\n\n\n363 rows √ó 1 columns\n\n\n\n\nfrom siuba.dply.vector import n_distinct\n(prs_df\n    &gt;&gt; select(_.repository_id, _.user_id)\n    &gt;&gt; distinct()\n    &gt;&gt; group_by(_.repository_id)\n    &gt;&gt; mutate(n_pr_makers = n_distinct(_.user_id))\n    &gt;&gt; ungroup()\n    &gt;&gt; select(_.repository_id, _.n_pr_makers)\n    &gt;&gt; distinct()\n    &gt;&gt; arrange(-_.n_pr_makers)\n)\n\n\n\n\n\n\n\n\nrepository_id\nn_pr_makers\n\n\n\n\n85\nMDEwOlJlcG9zaXRvcnkxMzYyMDI2OTU=\n725\n\n\n8\nMDEwOlJlcG9zaXRvcnkxMDc5Mzc4MTU=\n565\n\n\n23\nMDEwOlJlcG9zaXRvcnkxMzMxMDA4ODA=\n434\n\n\n7\nMDEwOlJlcG9zaXRvcnkxMDMwNzE1MjA=\n398\n\n\n11\nMDEwOlJlcG9zaXRvcnkxMzE2MTk2NDY=\n303\n\n\n...\n...\n...\n\n\n327\nR_kgDOHJgXCw\n1\n\n\n328\nR_kgDOHozdZg\n1\n\n\n329\nR_kgDOH9bggw\n1\n\n\n331\nMDEwOlJlcG9zaXRvcnkzODk0MzQwOTA=\n1\n\n\n332\nR_kgDOIOHC-A\n1\n\n\n\n\n333 rows √ó 2 columns\n\n\n\n\nlen(prs_df\n    &gt;&gt; distinct(_.repository_id)\n)\n\n333\n\n\n\nprs_df['created_at'] = prs_df.created_at.astype(\"datetime64[ns]\")\n# MLFLOW\n# (prs_df\n#     &gt;&gt; mutate(\n#         month = _.created_at.dt.month,\n#         year = _.created_at.dt.year\n#     )\n#     &gt;&gt; group_by(_.repository_id, _.month, _.year)\n#     &gt;&gt; count(_.month, _.year)\n#     &gt;&gt; filter(_.repository_id == 'MDEwOlJlcG9zaXRvcnkxMzYyMDI2OTU=')\n#     &gt;&gt; arrange(_.year, _.month)\n# )\n\n\n(prs_df\n    &gt;&gt; mutate(\n        year_month = _.created_at.dt.strftime('%Y-%m')\n    )\n    &gt;&gt; filter(_.year_month &lt; '2023-01')\n    &gt;&gt; count(_.year_month)\n    &gt;&gt; ggplot(aes(x = 'year_month', y = 'n'))\n        + geom_point()\n        + labs(x='Date', y='Number of pull requests')\n        + scale_x_date(date_breaks = '1 year')\n        + theme_matplotlib()\n        + theme(\n            figure_size=(10,5),\n            axis_text_x=element_text(margin={'t': 5, 'r': 5})\n        )\n        + labs(\n            title=\"Gross product pull requests for all repositories\"\n        )\n)\n\n\n\n\n\nrefined_df &gt;&gt; filter(_.full_name == 'mlflow/mlflow')\n\n\n\n\n\n\n\n\nid\nnode_id\nname\nfull_name\nowner\ndescription\ncreated_at\nupdated_at\npushed_at\nstargazers_count\n...\nhas_wiki\nhas_pages\nhas_discussions\nopen_issues_count\nlicense\nallow_forking\nis_template\ntopics\nage_days\ntime_since_last_commit_days\n\n\n\n\n100\n136202695\nMDEwOlJlcG9zaXRvcnkxMzYyMDI2OTU=\nmlflow\nmlflow/mlflow\n{'login': 'mlflow', 'id': 39938107, 'node_id':...\nOpen source platform for the machine learning ...\n2018-06-05 16:05:58\n2023-01-18T18:35:07Z\n2023-01-19 00:47:53\n13455\n...\nTrue\nFalse\nTrue\n1008\napache-2.0\nTrue\nFalse\n[ai, apache-spark, machine-learning, ml, mlflo...\n1690\n1\n\n\n\n\n1 rows √ó 24 columns\n\n\n\n\nlen(prs_df)\n\n121704\n\n\n\n(prs_df  \n    &gt;&gt; filter(_.state == 'MERGED')\n    &gt;&gt; count()\n)\n\n\n\n\n\n\n\n\nn\n\n\n\n\n0\n100836\n\n\n\n\n\n\n\n\nprs_small = (prs_df\n    &gt;&gt; filter(_.state == 'MERGED')\n    &gt;&gt; select(_.repository_id, _.title)\n)\n\n\nfrom sklearn.feature_extraction import text\n\nadditional_stopwords = frozenset({\n    \"readme\",\n    \"md\",\n    \"update\",\n    \"fix\",\n    \"fixing\",\n    \"fixes\",\n    \"docs\",\n    \"add\",\n    \"updated\",\n    \"feat\",\n    \"page\",\n    \"typo\",\n    \"ci\",\n    \"cd\",\n    \"github\",\n    \"chore\",\n    \"tests\",\n    \"test\",\n    \"feature\",\n    \"updates\",\n    \"links\",\n    \"merge\",\n    \"dev\",\n    \"broken\",\n    \"example\",\n    \"cli\",\n    \"update\",\n    \"deps\",\n    \"migration\",\n    \"lint\",\n    \"setup\",\n    \"link\",\n    \"update\",\n    \"error\",\n    \"message\",\n    \"remove\",\n    \"deps\",\n    \"update\",\n    \"dependency\",\n    \"dependencies\",\n    \"requirements\",\n    \"chore\",\n    \"v1\",\n    \"components\",\n    \"merge\",\n    \"0\",\n    \"1\",\n    \"bump\",\n    \"2\",\n    \"3\",\n    \"release\",\n    \"version\",\n    \"v0\",\n    \"build\",\n    \"support\",\n    \"refactor\",\n    \"type\",\n    \"types\",\n    \"coverage\",\n    \"master\",\n    \"main\",\n    \"branch\",\n    \"clean\",\n    \"documentation\",\n    \"testing\"\n})\n\nstop_words = text.ENGLISH_STOP_WORDS.union(additional_stopwords)\n\n\n# copied from 03_clean_text.ipynb\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\ntokenizer = RegexpTokenizer(r'\\w+')\n\npr_sents  = prs_small.title.to_list()\n# Vectorize document using TF-IDF\npr_tfidf = TfidfVectorizer(lowercase=True,\n                        stop_words=stop_words,\n                        ngram_range = (1,1),\n                        tokenizer = tokenizer.tokenize)\n\n# Fit and Transform the documents\npr_train_data = pr_tfidf.fit_transform(pr_sents)   \n# Define the number of topics or components\nnum_components=5\n\n# Create LDA object\npr_model=LatentDirichletAllocation(n_components=num_components)\n\n# Fit and Transform SVD model on data\npr_lda_matrix = pr_model.fit_transform(pr_train_data)\n\n# Get Components \npr_lda_components=pr_model.components_\n\n\n# Print the topics with their terms\npr_terms = pr_tfidf.get_feature_names_out()\n\nfor index, component in enumerate(pr_lda_components):\n    zipped = zip(pr_terms, component)\n    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:7]\n    top_terms_list=list(dict(top_terms_key).keys())\n    print(\"Topic \"+str(index)+\": \",top_terms_list)\n\nTopic 0:  ['7', 'doc', 'python', 'examples', 'actions', 'maintenance', 'model']\nTopic 1:  ['5', '4', '6', 'docker', '9', 'commit', 'sdk']\nTopic 2:  ['eslint', 'plugin', 'cleanup', 'monorepo', 'spark', 'v4', 'v5']\nTopic 3:  ['changelog', 'mlflow', 'helm', 'sample', 'small', 'added', 'use']\nTopic 4:  ['api', 'run', 'make', 'integration', 'dagit', 'model', 'sdk']\n\n\n\nimport matplotlib.pyplot as plt\n\ndef plot_top_words(model, feature_names, n_top_words, title):\n    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n    axes = axes.flatten()\n    for topic_idx, topic in enumerate(model.components_):\n        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n        top_features = [feature_names[i] for i in top_features_ind]\n        weights = topic[top_features_ind]\n\n        ax = axes[topic_idx]\n        ax.barh(top_features, weights, height=0.7)\n        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n        ax.invert_yaxis()\n        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n        for i in \"top right left\".split():\n            ax.spines[i].set_visible(False)\n        fig.suptitle(title, fontsize=40)\n\n    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n    plt.show()\n\nplot_top_words(pr_model, pr_tfidf.get_feature_names_out(), 20, \"top 10 topics\")\n\n\n\n\nokay, so all the topics are around maintainance, not necessarily features or mlops-y tasks. this is pretty expected"
  },
  {
    "objectID": "project/03_feature_engineering.html#what-about-issues",
    "href": "project/03_feature_engineering.html#what-about-issues",
    "title": "find relevant columns with correlation matricies",
    "section": "what about issues?",
    "text": "what about issues?\nlooking at issues titles, not body\n\nissues_df = pd.DataFrame(issues_total)\nissues_df.head()\n\n\n\n\n\n\n\n\nid\nmilestone_id\nrepository_id\nuser_id\nbody\nclosed_at\ncreated_at\nlocked\nnumber\nstate\ntitle\nupdated_at\ntype\n\n\n\n\n0\nMDU6SXNzdWUzODk2NzE0MDM=\nNone\nMDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\nMDQ6VXNlcjExNTMwMjQ3\nGreat repo!\\r\\nCan i translate it to Chinese?\n2018-12-11T11:14:53Z\n2018-12-11T09:37:03Z\nFalse\n69\nCLOSED\nChinese translation\n2022-09-16T08:27:39Z\nIssue\n\n\n1\nMDU6SXNzdWUzOTA0OTEwMzg=\nNone\nMDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\nMDQ6VXNlcjIyOTU5MDM3\nIs it okay to contribute to segmentation part ...\n2018-12-13T16:22:16Z\n2018-12-13T02:25:32Z\nFalse\n76\nCLOSED\nContribution to Computer Vision?\n2018-12-13T16:22:16Z\nIssue\n\n\n2\nMDU6SXNzdWUzOTA1NDQ4OTM=\nNone\nMDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\nMDQ6VXNlcjI3MTQzMDU4\nCould u pls release a instruction on Jupyter n...\n2018-12-13T13:33:46Z\n2018-12-13T07:05:19Z\nFalse\n78\nCLOSED\nNotebook\n2018-12-13T13:33:46Z\nIssue\n\n\n3\nMDU6SXNzdWUzOTA3NDQ3OTQ=\nNone\nMDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\nMDQ6VXNlcjcwMDYxNw==\nIn the [numpy notebook](https://github.com/Gok...\n2018-12-13T16:22:02Z\n2018-12-13T16:00:06Z\nFalse\n83\nCLOSED\n3d or 2d numpy array?\n2018-12-13T16:22:02Z\nIssue\n\n\n4\nMDU6SXNzdWUzOTI3NTY3OTc=\nNone\nMDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\nMDQ6VXNlcjExNTU1NzM=\nHi @GokuMohandas,\\r\\n\\r\\nI've been recently ta...\n2018-12-25T20:42:17Z\n2018-12-19T20:02:14Z\nFalse\n90\nCLOSED\nAlternative to Colab and Binder for running `p...\n2018-12-26T15:44:03Z\nIssue\n\n\n\n\n\n\n\n\nlen(issues_df)\n\n55994\n\n\n\nissues_sents  = issues_df.title.to_list()\n# Vectorize document using TF-IDF\nissues_tfidf = TfidfVectorizer(lowercase=True,\n                        stop_words=stop_words,\n                        ngram_range = (1,1),\n                        tokenizer = tokenizer.tokenize)\n\n# Fit and Transform the documents\nissues_train_data = issues_tfidf.fit_transform(pr_sents)   \n# Define the number of topics or components\nnum_components=5\n\n# Create LDA object\nissues_model=LatentDirichletAllocation(n_components=num_components)\n\n# Fit and Transform SVD model on data\nissues_lda_matrix = issues_model.fit_transform(pr_train_data)\n\n# Get Components \nissues_lda_components = issues_model.components_\n\n\n# Print the topics with their terms\nissues_terms = issues_tfidf.get_feature_names_out()\n\nfor index, component in enumerate(issues_lda_components):\n    zipped = zip(issues_terms, component)\n    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:7]\n    top_terms_list=list(dict(top_terms_key).keys())\n    print(\"Topic \"+str(index)+\": \",top_terms_list)\n\nTopic 0:  ['7', 'commit', 'pre', 'pytorch', 'sdk', 'examples', 'helm']\nTopic 1:  ['pipeline', 'api', 'det', 'use', 'sdk', 'node', 'default']\nTopic 2:  ['changelog', 'react', 'minor', 'plugin', 'eslint', 'v3', 'module']\nTopic 3:  ['4', '5', '6', 'python', '8', 'docker', 'monorepo']\nTopic 4:  ['doc', 'run', 'config', 'ui', 'api', 'store', 'command']\n\n\n\nfrom mizani.breaks import date_breaks\nfrom mizani.formatters import date_format\n(repo_df\n    &gt;&gt; select(_.created_at)\n    &gt;&gt; mutate(\n        created_at = _.created_at.astype(\"datetime64[ns]\"),\n        n = 1\n    )\n    &gt;&gt; arrange(_.created_at)\n    &gt;&gt; mutate(\n        n_cumsum = _.n.cumsum()\n    )\n    &gt;&gt; ggplot()\n        + geom_line(aes('created_at', 'n_cumsum'))\n        + scale_x_datetime(breaks=date_breaks('1 year'), labels=date_format('%Y'))     # modified\n        + labs(\n            x = 'Date',\n            y= 'Number of repositories',\n            title = 'Number of repositories with MLOps labels')\n        + theme_minimal()\n\n)"
  },
  {
    "objectID": "project/00_scrape_data.html",
    "href": "project/00_scrape_data.html",
    "title": "",
    "section": "",
    "text": "import json\n\ndef make_file_list() -&gt; list:\n    files = list()\n    i = 1\n    while i &lt; 11:\n        files.append('../data/repos/topic_mlops_pg_'+str(i)+'.json')\n        if i &lt; 2:\n            files.append('../data/repos/topic_modelmanagement_pg_'+str(i)+'.json')\n        i += 1\n    return files\n\ndef merge_files(files: list):\n    result = list()\n    for file in files:\n        with open(file, 'r') as input_file:\n            result.extend(json.load(input_file).get(\"items\"))\n\n    with open('../data/repos/topic_combined.json', 'w') as output_file:\n        json.dump(result, output_file)\n\nmerge_files(make_file_list())\nwith open(\"../data/repos/topic_combined.json\", \"r\") as read_file:\n    raw = json.load(read_file)\n\n\nassert len(raw) == 1044\n\n\nimport pandas as pd\nfrom siuba import *\n\ndf = pd.DataFrame(raw)\n\n\n(df\n    &gt;&gt; select(-_[\"url\":\"deployments_url\"])\n    &gt;&gt; filter(_.fork == False, _.private == False, _.open_issues &gt; 0, _.license is not None)\n    &gt;&gt; separate(_.full_name, [\"org_name\", \"repo_name\"], sep='/')\n)\n\n\n\n\n\n\n\n\nid\nnode_id\nname\nprivate\nowner\nhtml_url\ndescription\nfork\ncreated_at\nupdated_at\n...\ntopics\nvisibility\nforks\nopen_issues\nwatchers\ndefault_branch\npermissions\nscore\norg_name\nrepo_name\n\n\n\n\n0\n156157055\nMDEwOlJlcG9zaXRvcnkxNTYxNTcwNTU=\nMade-With-ML\nFalse\n{'login': 'GokuMohandas', 'id': 8000987, 'node...\nhttps://github.com/GokuMohandas/Made-With-ML\nLearn how to responsibly develop, deploy and m...\nFalse\n2018-11-05T03:44:27Z\n2023-01-18T14:24:54Z\n...\n[data-engineering, data-science, deep-learning...\npublic\n5308\n6\n32107\nmain\n{'admin': False, 'maintain': False, 'push': Fa...\n1.0\nGokuMohandas\nMade-With-ML\n\n\n1\n240315046\nMDEwOlJlcG9zaXRvcnkyNDAzMTUwNDY=\njina\nFalse\n{'login': 'jina-ai', 'id': 60539444, 'node_id'...\nhttps://github.com/jina-ai/jina\nüîÆ Build multimodal AI services via cloud nativ...\nFalse\n2020-02-13T17:04:44Z\n2023-01-18T14:18:14Z\n...\n[aiops, airflow, cloud-native, creative-ai, cr...\npublic\n2012\n33\n17144\nmaster\n{'admin': False, 'maintain': False, 'push': Fa...\n1.0\njina-ai\njina\n\n\n2\n144863525\nMDEwOlJlcG9zaXRvcnkxNDQ4NjM1MjU=\nawesome-production-machine-learning\nFalse\n{'login': 'EthicalML', 'id': 43532924, 'node_i...\nhttps://github.com/EthicalML/awesome-productio...\nA curated list of awesome open source librarie...\nFalse\n2018-08-15T14:28:41Z\n2023-01-18T17:16:27Z\n...\n[awesome, awesome-list, data-mining, deep-lear...\npublic\n1817\n26\n13008\nmaster\n{'admin': False, 'maintain': False, 'push': Fa...\n1.0\nEthicalML\nawesome-production-machine-learning\n\n\n3\n135673451\nMDEwOlJlcG9zaXRvcnkxMzU2NzM0NTE=\nnni\nFalse\n{'login': 'microsoft', 'id': 6154722, 'node_id...\nhttps://github.com/microsoft/nni\nAn open source AutoML toolkit for automate mac...\nFalse\n2018-06-01T05:51:44Z\n2023-01-18T12:49:05Z\n...\n[automated-machine-learning, automl, bayesian-...\npublic\n1736\n290\n12415\nmaster\n{'admin': False, 'maintain': False, 'push': Fa...\n1.0\nmicrosoft\nnni\n\n\n4\n192640529\nMDEwOlJlcG9zaXRvcnkxOTI2NDA1Mjk=\nlabel-studio\nFalse\n{'login': 'heartexlabs', 'id': 48309720, 'node...\nhttps://github.com/heartexlabs/label-studio\nLabel Studio is a multi-type data labeling and...\nFalse\n2019-06-19T02:00:44Z\n2023-01-18T11:31:18Z\n...\n[annotation, annotation-tool, annotations, bou...\npublic\n1362\n448\n11747\ndevelop\n{'admin': False, 'maintain': False, 'push': Fa...\n1.0\nheartexlabs\nlabel-studio\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1005\n534175875\nR_kgDOH9bggw\nlightning-hydra-template\nFalse\n{'login': 'aiplaybookin', 'id': 83638338, 'nod...\nhttps://github.com/aiplaybookin/lightning-hydr...\nML/DL Template with Hydra - DVC, Hyperparamete...\nFalse\n2022-09-08T11:15:13Z\n2022-12-03T14:02:39Z\n...\n[deep-learning, machine-learning, mlops, mlops...\npublic\n0\n5\n0\nmain\n{'admin': False, 'maintain': False, 'push': Fa...\n1.0\naiplaybookin\nlightning-hydra-template\n\n\n1008\n336675932\nMDEwOlJlcG9zaXRvcnkzMzY2NzU5MzI=\nairjordan\nFalse\n{'login': 'kevinmchan', 'id': 2401363, 'node_i...\nhttps://github.com/kevinmchan/airjordan\nAn airflow pipeline for building and scoring N...\nFalse\n2021-02-07T01:40:15Z\n2021-03-29T03:47:21Z\n...\n[airflow, etl-pipeline, ml, mlops, nba]\npublic\n0\n13\n0\nmain\n{'admin': False, 'maintain': False, 'push': Fa...\n1.0\nkevinmchan\nairjordan\n\n\n1015\n477140264\nR_kgDOHHCVKA\nDataWorkstation.jl\nFalse\n{'login': 'leferrad', 'id': 10536218, 'node_id...\nhttps://github.com/leferrad/DataWorkstation.jl\nA Julia framework to produce your data project...\nFalse\n2022-04-02T18:34:24Z\n2022-08-14T20:34:58Z\n...\n[data-science, julia, machine-learning, mlops,...\npublic\n0\n1\n0\nmain\n{'admin': False, 'maintain': False, 'push': Fa...\n1.0\nleferrad\nDataWorkstation.jl\n\n\n1026\n389434090\nMDEwOlJlcG9zaXRvcnkzODk0MzQwOTA=\nRossmann-Pharmaceuticals-Sales-prediction\nFalse\n{'login': 'Azariagmt', 'id': 56393921, 'node_i...\nhttps://github.com/Azariagmt/Rossmann-Pharmace...\nTime series sales forecast for Rossmann Pharma...\nFalse\n2021-07-25T20:19:38Z\n2021-08-04T11:02:24Z\n...\n[data-analysis, mlops, regression, rossmann, t...\npublic\n0\n1\n0\nmain\n{'admin': False, 'maintain': False, 'push': Fa...\n1.0\nAzariagmt\nRossmann-Pharmaceuticals-Sales-prediction\n\n\n1038\n551666424\nR_kgDOIOHC-A\nmlops\nFalse\n{'login': 'paulsilcock', 'id': 12411602, 'node...\nhttps://github.com/paulsilcock/mlops\nA scratch space to experiment with MLOps techn...\nFalse\n2022-10-14T21:10:03Z\n2022-10-29T15:14:53Z\n...\n[argo-workflows, docker, dvc, github-actions, ...\npublic\n0\n3\n0\nmain\n{'admin': False, 'maintain': False, 'push': Fa...\n1.0\npaulsilcock\nmlops\n\n\n\n\n368 rows √ó 45 columns\n\n\n\nScope: looking at just tools.\nHow to prove if something is an OSS tool?\n\nhas a license\nhas at least one open issue (probably not a strong criteria)\nis public\n\nthink of removing\n\nlists such as ‚Äúawesome mlops‚Äù esque\nprojects (will probably be at least partially removed with no license?)\n\nWhat other tags to collect?\n\nmlops\nmodel-management\n\nOther metrics to bring in?\n\ncontributors"
  },
  {
    "objectID": "proposal/index.html",
    "href": "proposal/index.html",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "",
    "text": "Machine learning operations, or MLOps, is a set of practices to deploy and maintain machine learning models in production reliably and efficiently. This paper will explore the open source MLOps ecosystem as a whole and highlight tools of interest. MLOps tools will be compared and classified by task and maturity. Finally, there will be an overview of key players in the space. It seeks to highlight the knowledge necessary for practitioners and academics to select an open source machine learning operations tool (or tools) to suit their data science projects and machine learning systems."
  },
  {
    "objectID": "proposal/index.html#the-interdisciplinary-nature-of-proposed-research",
    "href": "proposal/index.html#the-interdisciplinary-nature-of-proposed-research",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "The interdisciplinary nature of proposed research",
    "text": "The interdisciplinary nature of proposed research\nMachine learning operations have a naturally interdisciplinary nature. In fact, many of the reasons effectively implementing MLOps solutions is so difficult is due to the span of knowledge necessary for successful operationalization of models. In researching this topic, it is necessary to have knowledge of data wrangling and statistical modeling to create the model (as taught in CAP 5320 - Data Wrangling and Exploratory Data Analysis and CAP 5765 - Computational Data Analysis), along with knowledge of software engineering best practices to have a stable deployment (CEN 5035 - Advanced Software Engineering) and visualization skills to continually monitor model performance (CAP 5735 - Data Visualization and Reproducible Research)."
  },
  {
    "objectID": "proposal/index.html#expected-outcomes",
    "href": "proposal/index.html#expected-outcomes",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Expected outcomes",
    "text": "Expected outcomes\n\nResearch paper\nInteractive dashboard\n\nThrough the expected outcomes, the implications of machine learning operations in open source software will be clear. The paper will lay out the types of tools available for operationalization of models, clustering of types of tools, and an analysis of the maturity of the ecosystem. The dashboard will give an interactive way to explore the data and maturity of the machine learning operations open source ecosystem."
  },
  {
    "objectID": "proposal/index.html#objective-timeline",
    "href": "proposal/index.html#objective-timeline",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Objective timeline",
    "text": "Objective timeline\nAs seen in Figure 2, this project will be broken up into the following parts: literature review, prepare project proposal, gather data, clean and explore data, build maturity models, build unsupervised learning models, build interactive dashboard, and write final report. The literature review and project proposal will be finished by December 2023. Starting in December 2022, data collection of many open source projects will begin. The first task will be to curate a list of projects, sources such as blog posts and GitHub tags will be used. From this list of repositories, further data on these tools can be scraped from the GitHub API. This data will give information such as: description, stars, number of contributors, forks, language, open issues, etc. Once gathered, exploratory data analysis is expected to begin January 2023e. By mid-February, it is expected to be starting on building unsupervised learning models for categorizing tools, maturity models for determining maturity, and a dashboard for an interactive experience to explore the data and models. Starting in March, the final report will be written, to be presented late-April.\n\n\nCode\nimport pandas as pd\nfrom siuba import *\nfrom plotnine import *\n\nobjectives = pd.DataFrame({\n  'task': ['review relevant literature', 'prepare project proposal', 'gather data', 'data cleaning and exploration', 'build maturity models', 'build unsupervised learning models', 'build interactive dashboard', 'author final report'],\n  'start': pd.to_datetime(['2022-08-30', '2022-10-15', '2022-12-01', '2023-01-09', '2023-02-27', '2023-02-27', '2023-03-01', '2023-03-17']),\n  'end': pd.to_datetime(['2022-11-30', '2022-11-30', '2023-01-23', '2023-02-27', '2023-03-27', '2023-03-27', '2023-03-27', '2023-04-17'])\n})\n\n(objectives\n  &gt;&gt; ggplot(aes(x='start', xend='end', y='task', yend='task'))\n    + geom_segment(size=15, color='#a7a9ac')\n    + xlab('date')\n    + ylab('')\n    + ggtitle('Timeline of project completion')\n    + theme_tufte()\n    + theme(axis_text_x=element_text(angle=15))\n  )\n\n\n\n\n\nFigure 2: Project Gantt chart"
  },
  {
    "objectID": "proposal/index.html#further-work",
    "href": "proposal/index.html#further-work",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Further work",
    "text": "Further work\nThe discoveries from this work could easily be extended further. One possible outcome could be an opinionated workflow for a specific task built with a selection of the tools analyzed; this workflow would contribute to these open source tools through documentation and tutorials, if not also code contributions. It is possible that this software stack could be used to build an accompanying case study, either to be included inside this paper or as part of another work. Another outcome could be a second paper in the style of ‚ÄúTen simple rules for X‚Äù, where the topic would be related to machine learning operations."
  },
  {
    "objectID": "proposal/proposal.html#section",
    "href": "proposal/proposal.html#section",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "",
    "text": "MLOps, is a set of practices to deploy and maintain machine learning models in production reliably and efficiently.\n\nand that is partially because it is so new! if we use google trends, we can see that in late 2021 ish, google searches for mlops started to increase‚Äì\nhowever, academic papers on mlops are still scarce, if you were to search google scholar for mlops, you would only get about 1,500 results, where machine learning operations has well over 3.5 million.\nmachine learning operations is important‚Äì they are a set of practices to ‚Ä¶\nbringing machine learning models outside of local development into a production environment is becoming even more important as machine learning models are being integrated in many services that affect our everyday lives"
  },
  {
    "objectID": "proposal/proposal.html#expected-outcomes",
    "href": "proposal/proposal.html#expected-outcomes",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Expected outcomes",
    "text": "Expected outcomes\nThis project will explore the open source machine learning operations ecosystem as a whole and highlight tools of interest. MLOps tools will be compared and classified by task and maturity.\n\n\nResearch paper\nInteractive dashboard\n\n\ntasks will be classified by unsupervised learning methods\n\norchestration\nml platform\nmodel versioning\nmodel services\nmodel performance monitoring\n\nmaturity will be proxied by github interactions"
  },
  {
    "objectID": "proposal/proposal.html#steps",
    "href": "proposal/proposal.html#steps",
    "title": "Meta-analysis of the machine learning operations open source ecosystem",
    "section": "Steps",
    "text": "Steps\n\n\nGather data\nClean and explore data\nBuild maturity models\nBuild unsupervised learning models\nBuild interactive dashboard"
  }
]